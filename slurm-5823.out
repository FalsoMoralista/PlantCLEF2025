INFO:root:called-params config/configs.yaml
INFO:root:loaded params...
INFO:root:{'experiment_code': 2, 'k_means': {'K': 7806}, 'patches': {'N': 64}, 'num_workers': 16, 'base_dir': '/home/rtcalumby/adam/luciano/PlantCLEF2025/PlantCLEF2025/', 'pretrained_path': 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar', 'data': {'class_mapping': 'pretrained_models/class_mapping.txt', 'species_mapping': 'pretrained_models/species_id_to_name.txt', 'test_data': '/home/rtcalumby/adam/luciano/PlantCLEF2025/test_dataset/'}, 'batch_size': 1, 'gradient_accumulation': 128, 'optimization': {'epochs': 120, 'final_lr': 1e-06, 'final_weight_decay': 0.4, 'ipe_scale': 1.0, 'lr': 0.001, 'start_lr': 1e-05, 'warmup': 15, 'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/2)
INFO:root:Initialized (rank/world-size) 0/2
INFO:timm.models._helpers:Loaded state_dict_ema from checkpoint 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar'
Test dataset created for rank 0 world_size:  2
Test dataset, length: 1053
INFO:root:Loading Vision Transformer: PilotVisionTransformer(
  (patch_embed): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (patch_drop): Identity()
    (norm_pre): Identity()
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (fc_norm): Identity()
    (head_drop): Dropout(p=0.0, inplace=False)
    (head): Identity()
  )
  (linear): Linear(in_features=1536, out_features=7806, bias=True)
  (blocks): ModuleList(
    (0-5): 6 x Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
INFO:root:Using AdamW
Allocated Memory with model loading: 1.0564966201782227  GB
INFO:root:Epoch 1
Rank 1
Test dataset created for rank 1 world_size:  2
Test dataset, length: 1053
Allocated Memory with model loading: 1.0564966201782227  GB
INFO:root:[1,     0/ 1053] - train_loss: 0.8508 -[wd: 1.00e-02] [lr: 1.00e-03][max mem allocated: 5.55e+04] (7749.4 ms)
INFO:root:[1,   128/ 1053] - train_loss: 0.8515 -[wd: 4.00e-02] [lr: 1.01e-05][max mem allocated: 5.59e+04] (5252.7 ms)
INFO:root:[1,   256/ 1053] - train_loss: 0.8455 -[wd: 4.00e-02] [lr: 1.01e-05][max mem allocated: 5.61e+04] (5740.8 ms)
INFO:root:[1,   384/ 1053] - train_loss: 0.8395 -[wd: 4.00e-02] [lr: 1.02e-05][max mem allocated: 5.61e+04] (5730.3 ms)
INFO:root:[1,   512/ 1053] - train_loss: 0.8334 -[wd: 4.00e-02] [lr: 1.03e-05][max mem allocated: 5.61e+04] (5725.3 ms)
INFO:root:[1,   640/ 1053] - train_loss: 0.8272 -[wd: 4.00e-02] [lr: 1.03e-05][max mem allocated: 5.61e+04] (5720.7 ms)
INFO:root:[1,   768/ 1053] - train_loss: 0.8209 -[wd: 4.00e-02] [lr: 1.04e-05][max mem allocated: 5.61e+04] (5717.6 ms)
INFO:root:[1,   896/ 1053] - train_loss: 0.8143 -[wd: 4.00e-02] [lr: 1.04e-05][max mem allocated: 5.61e+04] (5715.4 ms)
INFO:root:[1,  1024/ 1053] - train_loss: 0.8076 -[wd: 4.00e-02] [lr: 1.05e-05][max mem allocated: 5.61e+04] (5713.0 ms)
INFO:root:Loss 0.7490
INFO:root:Epoch 2
INFO:root:[2,     0/ 1053] - train_loss: 0.7392 -[wd: 4.00e-02] [lr: 1.05e-05][max mem allocated: 5.61e+04] (5620.0 ms)
INFO:root:[2,   128/ 1053] - train_loss: 0.7453 -[wd: 4.00e-02] [lr: 1.06e-05][max mem allocated: 5.61e+04] (5653.9 ms)
INFO:root:[2,   256/ 1053] - train_loss: 0.7377 -[wd: 4.00e-02] [lr: 1.06e-05][max mem allocated: 5.61e+04] (5660.9 ms)
INFO:root:[2,   384/ 1053] - train_loss: 0.7302 -[wd: 4.00e-02] [lr: 1.07e-05][max mem allocated: 5.61e+04] (5664.8 ms)
INFO:root:[2,   512/ 1053] - train_loss: 0.7225 -[wd: 4.00e-02] [lr: 1.08e-05][max mem allocated: 5.61e+04] (5668.5 ms)
INFO:root:[2,   640/ 1053] - train_loss: 0.7146 -[wd: 4.00e-02] [lr: 1.08e-05][max mem allocated: 5.61e+04] (5670.5 ms)
INFO:root:[2,   768/ 1053] - train_loss: 0.7067 -[wd: 4.00e-02] [lr: 1.09e-05][max mem allocated: 5.61e+04] (5670.5 ms)
INFO:root:[2,   896/ 1053] - train_loss: 0.6988 -[wd: 4.00e-02] [lr: 1.09e-05][max mem allocated: 5.61e+04] (5671.1 ms)
INFO:root:[2,  1024/ 1053] - train_loss: 0.6910 -[wd: 4.00e-02] [lr: 1.10e-05][max mem allocated: 5.61e+04] (5672.4 ms)
INFO:root:Loss 0.6245
INFO:root:Epoch 3
INFO:root:[3,     0/ 1053] - train_loss: 0.6199 -[wd: 4.00e-02] [lr: 1.10e-05][max mem allocated: 5.61e+04] (5246.3 ms)
INFO:root:[3,   128/ 1053] - train_loss: 0.6206 -[wd: 4.00e-02] [lr: 1.11e-05][max mem allocated: 5.61e+04] (5665.1 ms)
INFO:root:[3,   256/ 1053] - train_loss: 0.6132 -[wd: 4.00e-02] [lr: 1.11e-05][max mem allocated: 5.61e+04] (5669.9 ms)
INFO:root:[3,   384/ 1053] - train_loss: 0.6058 -[wd: 4.00e-02] [lr: 1.12e-05][max mem allocated: 5.61e+04] (5670.7 ms)
INFO:root:[3,   512/ 1053] - train_loss: 0.5986 -[wd: 4.00e-02] [lr: 1.13e-05][max mem allocated: 5.61e+04] (5671.8 ms)
INFO:root:[3,   640/ 1053] - train_loss: 0.5913 -[wd: 4.00e-02] [lr: 1.13e-05][max mem allocated: 5.61e+04] (5673.0 ms)
INFO:root:[3,   768/ 1053] - train_loss: 0.5844 -[wd: 4.00e-02] [lr: 1.14e-05][max mem allocated: 5.61e+04] (5673.4 ms)
INFO:root:[3,   896/ 1053] - train_loss: 0.5774 -[wd: 4.00e-02] [lr: 1.14e-05][max mem allocated: 5.61e+04] (5674.9 ms)
INFO:root:[3,  1024/ 1053] - train_loss: 0.5706 -[wd: 4.00e-02] [lr: 1.15e-05][max mem allocated: 5.61e+04] (5676.6 ms)
INFO:root:Loss 0.5091
INFO:root:Epoch 4
INFO:root:[4,     0/ 1053] - train_loss: 0.5114 -[wd: 4.00e-02] [lr: 1.15e-05][max mem allocated: 5.61e+04] (5382.0 ms)
INFO:root:[4,   128/ 1053] - train_loss: 0.5108 -[wd: 4.00e-02] [lr: 1.16e-05][max mem allocated: 5.61e+04] (5658.1 ms)
INFO:root:[4,   256/ 1053] - train_loss: 0.5047 -[wd: 4.00e-02] [lr: 1.16e-05][max mem allocated: 5.61e+04] (5664.0 ms)
INFO:root:[4,   384/ 1053] - train_loss: 0.4989 -[wd: 4.00e-02] [lr: 1.17e-05][max mem allocated: 5.61e+04] (5669.6 ms)
INFO:root:[4,   512/ 1053] - train_loss: 0.4932 -[wd: 4.00e-02] [lr: 1.18e-05][max mem allocated: 5.61e+04] (5671.5 ms)
INFO:root:[4,   640/ 1053] - train_loss: 0.4876 -[wd: 4.00e-02] [lr: 1.18e-05][max mem allocated: 5.61e+04] (5673.1 ms)
INFO:root:[4,   768/ 1053] - train_loss: 0.4822 -[wd: 4.00e-02] [lr: 1.19e-05][max mem allocated: 5.61e+04] (5673.0 ms)
INFO:root:[4,   896/ 1053] - train_loss: 0.4769 -[wd: 4.00e-02] [lr: 1.19e-05][max mem allocated: 5.61e+04] (5674.9 ms)
INFO:root:[4,  1024/ 1053] - train_loss: 0.4717 -[wd: 4.00e-02] [lr: 1.20e-05][max mem allocated: 5.61e+04] (5675.2 ms)
INFO:root:Loss 0.4266
INFO:root:Epoch 5
INFO:root:[5,     0/ 1053] - train_loss: 0.4262 -[wd: 4.00e-02] [lr: 1.20e-05][max mem allocated: 5.61e+04] (7321.7 ms)
INFO:root:[5,   128/ 1053] - train_loss: 0.4264 -[wd: 4.00e-02] [lr: 1.21e-05][max mem allocated: 5.61e+04] (5688.0 ms)
INFO:root:[5,   256/ 1053] - train_loss: 0.4220 -[wd: 4.00e-02] [lr: 1.21e-05][max mem allocated: 5.61e+04] (5682.3 ms)
INFO:root:[5,   384/ 1053] - train_loss: 0.4178 -[wd: 4.00e-02] [lr: 1.22e-05][max mem allocated: 5.61e+04] (5678.2 ms)
INFO:root:[5,   512/ 1053] - train_loss: 0.4137 -[wd: 4.00e-02] [lr: 1.23e-05][max mem allocated: 5.61e+04] (5678.0 ms)
INFO:root:[5,   640/ 1053] - train_loss: 0.4096 -[wd: 4.00e-02] [lr: 1.23e-05][max mem allocated: 5.61e+04] (5678.5 ms)
INFO:root:[5,   768/ 1053] - train_loss: 0.4057 -[wd: 4.00e-02] [lr: 1.24e-05][max mem allocated: 5.61e+04] (5679.4 ms)
INFO:root:[5,   896/ 1053] - train_loss: 0.4019 -[wd: 4.00e-02] [lr: 1.24e-05][max mem allocated: 5.61e+04] (5680.5 ms)
INFO:root:[5,  1024/ 1053] - train_loss: 0.3982 -[wd: 4.00e-02] [lr: 1.25e-05][max mem allocated: 5.61e+04] (5681.0 ms)
INFO:root:Loss 0.3653
INFO:root:Epoch 6
INFO:root:[6,     0/ 1053] - train_loss: 0.3647 -[wd: 4.00e-02] [lr: 1.25e-05][max mem allocated: 5.61e+04] (5584.9 ms)
INFO:root:[6,   128/ 1053] - train_loss: 0.3658 -[wd: 4.00e-02] [lr: 1.26e-05][max mem allocated: 5.61e+04] (5710.9 ms)
INFO:root:[6,   256/ 1053] - train_loss: 0.3626 -[wd: 4.00e-02] [lr: 1.26e-05][max mem allocated: 5.61e+04] (5699.2 ms)
INFO:root:[6,   384/ 1053] - train_loss: 0.3596 -[wd: 4.00e-02] [lr: 1.27e-05][max mem allocated: 5.61e+04] (5701.1 ms)
INFO:root:[6,   512/ 1053] - train_loss: 0.3566 -[wd: 4.00e-02] [lr: 1.28e-05][max mem allocated: 5.61e+04] (5703.1 ms)
INFO:root:[6,   640/ 1053] - train_loss: 0.3538 -[wd: 4.00e-02] [lr: 1.28e-05][max mem allocated: 5.61e+04] (5704.2 ms)
INFO:root:[6,   768/ 1053] - train_loss: 0.3510 -[wd: 4.00e-02] [lr: 1.29e-05][max mem allocated: 5.61e+04] (5710.8 ms)
INFO:root:[6,   896/ 1053] - train_loss: 0.3483 -[wd: 4.00e-02] [lr: 1.29e-05][max mem allocated: 5.61e+04] (5706.0 ms)
INFO:root:[6,  1024/ 1053] - train_loss: 0.3457 -[wd: 4.00e-02] [lr: 1.30e-05][max mem allocated: 5.61e+04] (5708.0 ms)
INFO:root:Loss 0.3222
INFO:root:Epoch 7
INFO:root:[7,     0/ 1053] - train_loss: 0.3219 -[wd: 4.00e-02] [lr: 1.30e-05][max mem allocated: 5.61e+04] (5394.2 ms)
INFO:root:[7,   128/ 1053] - train_loss: 0.3230 -[wd: 4.00e-02] [lr: 1.31e-05][max mem allocated: 5.61e+04] (5696.4 ms)
INFO:root:[7,   256/ 1053] - train_loss: 0.3208 -[wd: 4.00e-02] [lr: 1.31e-05][max mem allocated: 5.61e+04] (5704.6 ms)
INFO:root:[7,   384/ 1053] - train_loss: 0.3187 -[wd: 4.00e-02] [lr: 1.32e-05][max mem allocated: 5.61e+04] (5701.1 ms)
INFO:root:[7,   512/ 1053] - train_loss: 0.3167 -[wd: 4.00e-02] [lr: 1.33e-05][max mem allocated: 5.61e+04] (5709.6 ms)
INFO:root:[7,   640/ 1053] - train_loss: 0.3147 -[wd: 4.00e-02] [lr: 1.33e-05][max mem allocated: 5.61e+04] (5711.1 ms)
INFO:root:[7,   768/ 1053] - train_loss: 0.3128 -[wd: 4.00e-02] [lr: 1.34e-05][max mem allocated: 5.61e+04] (5714.0 ms)
INFO:root:[7,   896/ 1053] - train_loss: 0.3109 -[wd: 4.00e-02] [lr: 1.34e-05][max mem allocated: 5.61e+04] (5714.4 ms)
INFO:root:[7,  1024/ 1053] - train_loss: 0.3091 -[wd: 4.00e-02] [lr: 1.35e-05][max mem allocated: 5.61e+04] (5714.3 ms)
INFO:root:Loss 0.2925
INFO:root:Epoch 8
INFO:root:[8,     0/ 1053] - train_loss: 0.2928 -[wd: 4.00e-02] [lr: 1.35e-05][max mem allocated: 5.61e+04] (5269.8 ms)
INFO:root:[8,   128/ 1053] - train_loss: 0.2934 -[wd: 4.00e-02] [lr: 1.36e-05][max mem allocated: 5.61e+04] (5698.4 ms)
INFO:root:[8,   256/ 1053] - train_loss: 0.2919 -[wd: 4.00e-02] [lr: 1.36e-05][max mem allocated: 5.61e+04] (5701.0 ms)
INFO:root:[8,   384/ 1053] - train_loss: 0.2905 -[wd: 4.00e-02] [lr: 1.37e-05][max mem allocated: 5.61e+04] (5700.1 ms)
INFO:root:[8,   512/ 1053] - train_loss: 0.2891 -[wd: 4.00e-02] [lr: 1.38e-05][max mem allocated: 5.61e+04] (5702.7 ms)
INFO:root:[8,   640/ 1053] - train_loss: 0.2877 -[wd: 4.00e-02] [lr: 1.38e-05][max mem allocated: 5.61e+04] (5709.5 ms)
INFO:root:[8,   768/ 1053] - train_loss: 0.2864 -[wd: 4.00e-02] [lr: 1.39e-05][max mem allocated: 5.61e+04] (5710.5 ms)
INFO:root:[8,   896/ 1053] - train_loss: 0.2852 -[wd: 4.00e-02] [lr: 1.39e-05][max mem allocated: 5.61e+04] (5710.6 ms)
INFO:root:[8,  1024/ 1053] - train_loss: 0.2839 -[wd: 4.00e-02] [lr: 1.40e-05][max mem allocated: 5.61e+04] (5710.4 ms)
INFO:root:Loss 0.2729
INFO:root:Epoch 9
INFO:root:[9,     0/ 1053] - train_loss: 0.2727 -[wd: 4.00e-02] [lr: 1.40e-05][max mem allocated: 5.61e+04] (6915.6 ms)
INFO:root:[9,   128/ 1053] - train_loss: 0.2732 -[wd: 4.00e-02] [lr: 1.41e-05][max mem allocated: 5.61e+04] (5716.9 ms)
INFO:root:[9,   256/ 1053] - train_loss: 0.2722 -[wd: 4.00e-02] [lr: 1.41e-05][max mem allocated: 5.61e+04] (5705.9 ms)
INFO:root:[9,   384/ 1053] - train_loss: 0.2712 -[wd: 4.00e-02] [lr: 1.42e-05][max mem allocated: 5.61e+04] (5697.3 ms)
INFO:root:[9,   512/ 1053] - train_loss: 0.2702 -[wd: 4.00e-02] [lr: 1.43e-05][max mem allocated: 5.61e+04] (5701.0 ms)
INFO:root:[9,   640/ 1053] - train_loss: 0.2694 -[wd: 4.00e-02] [lr: 1.43e-05][max mem allocated: 5.61e+04] (5705.4 ms)
INFO:root:[9,   768/ 1053] - train_loss: 0.2685 -[wd: 4.00e-02] [lr: 1.44e-05][max mem allocated: 5.61e+04] (5704.6 ms)
INFO:root:[9,   896/ 1053] - train_loss: 0.2676 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 5.61e+04] (5701.8 ms)
INFO:root:[9,  1024/ 1053] - train_loss: 0.2668 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 5.61e+04] (5695.7 ms)
INFO:root:Loss 0.2596
INFO:root:Epoch 10
INFO:root:[10,     0/ 1053] - train_loss: 0.2598 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 5.61e+04] (9264.4 ms)
INFO:root:[10,   128/ 1053] - train_loss: 0.2595 -[wd: 4.00e-02] [lr: 1.46e-05][max mem allocated: 5.61e+04] (5664.0 ms)
INFO:root:[10,   256/ 1053] - train_loss: 0.2589 -[wd: 4.00e-02] [lr: 1.46e-05][max mem allocated: 5.61e+04] (5674.1 ms)
INFO:root:[10,   384/ 1053] - train_loss: 0.2582 -[wd: 4.00e-02] [lr: 1.47e-05][max mem allocated: 5.61e+04] (5696.3 ms)
INFO:root:[10,   512/ 1053] - train_loss: 0.2575 -[wd: 4.00e-02] [lr: 1.48e-05][max mem allocated: 5.61e+04] (5688.4 ms)
INFO:root:[10,   640/ 1053] - train_loss: 0.2569 -[wd: 4.00e-02] [lr: 1.48e-05][max mem allocated: 5.61e+04] (5682.8 ms)
INFO:root:[10,   768/ 1053] - train_loss: 0.2563 -[wd: 4.00e-02] [lr: 1.49e-05][max mem allocated: 5.61e+04] (5681.8 ms)
INFO:root:[10,   896/ 1053] - train_loss: 0.2557 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 5.61e+04] (5688.5 ms)
INFO:root:[10,  1024/ 1053] - train_loss: 0.2552 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 5.61e+04] (5692.7 ms)
INFO:root:Loss 0.2503
INFO:root:Epoch 11
INFO:root:[11,     0/ 1053] - train_loss: 0.2508 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 5.61e+04] (6926.2 ms)
INFO:root:[11,   128/ 1053] - train_loss: 0.2503 -[wd: 4.00e-02] [lr: 1.51e-05][max mem allocated: 5.61e+04] (5665.5 ms)
INFO:root:[11,   256/ 1053] - train_loss: 0.2498 -[wd: 4.00e-02] [lr: 1.51e-05][max mem allocated: 5.61e+04] (5668.0 ms)
INFO:root:[11,   384/ 1053] - train_loss: 0.2494 -[wd: 4.00e-02] [lr: 1.52e-05][max mem allocated: 5.61e+04] (5674.3 ms)
INFO:root:[11,   512/ 1053] - train_loss: 0.2489 -[wd: 4.00e-02] [lr: 1.53e-05][max mem allocated: 5.61e+04] (5674.9 ms)
INFO:root:[11,   640/ 1053] - train_loss: 0.2485 -[wd: 4.00e-02] [lr: 1.53e-05][max mem allocated: 5.61e+04] (5675.3 ms)
INFO:root:[11,   768/ 1053] - train_loss: 0.2481 -[wd: 4.00e-02] [lr: 1.54e-05][max mem allocated: 5.61e+04] (5673.4 ms)
INFO:root:[11,   896/ 1053] - train_loss: 0.2477 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 5.61e+04] (5680.3 ms)
INFO:root:[11,  1024/ 1053] - train_loss: 0.2473 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 5.61e+04] (5683.2 ms)
INFO:root:Loss 0.2444
INFO:root:Epoch 12
INFO:root:[12,     0/ 1053] - train_loss: 0.2442 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 5.61e+04] (9763.8 ms)
INFO:root:[12,   128/ 1053] - train_loss: 0.2440 -[wd: 4.00e-02] [lr: 1.56e-05][max mem allocated: 5.61e+04] (5715.7 ms)
INFO:root:[12,   256/ 1053] - train_loss: 0.2437 -[wd: 4.00e-02] [lr: 1.56e-05][max mem allocated: 5.61e+04] (5695.2 ms)
INFO:root:[12,   384/ 1053] - train_loss: 0.2434 -[wd: 4.00e-02] [lr: 1.57e-05][max mem allocated: 5.61e+04] (5684.5 ms)
INFO:root:[12,   512/ 1053] - train_loss: 0.2431 -[wd: 4.00e-02] [lr: 1.58e-05][max mem allocated: 5.61e+04] (5678.8 ms)
INFO:root:[12,   640/ 1053] - train_loss: 0.2428 -[wd: 4.00e-02] [lr: 1.58e-05][max mem allocated: 5.61e+04] (5675.7 ms)
INFO:root:[12,   768/ 1053] - train_loss: 0.2426 -[wd: 4.00e-02] [lr: 1.59e-05][max mem allocated: 5.61e+04] (5677.4 ms)
INFO:root:[12,   896/ 1053] - train_loss: 0.2423 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 5.61e+04] (5680.6 ms)
INFO:root:[12,  1024/ 1053] - train_loss: 0.2420 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 5.61e+04] (5676.3 ms)
INFO:root:Loss 0.2398
INFO:root:Epoch 13
INFO:root:[13,     0/ 1053] - train_loss: 0.2399 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 5.61e+04] (5738.3 ms)
INFO:root:[13,   128/ 1053] - train_loss: 0.2397 -[wd: 4.00e-02] [lr: 1.61e-05][max mem allocated: 5.61e+04] (5674.0 ms)
INFO:root:[13,   256/ 1053] - train_loss: 0.2395 -[wd: 4.00e-02] [lr: 1.61e-05][max mem allocated: 5.61e+04] (5666.8 ms)
INFO:root:[13,   384/ 1053] - train_loss: 0.2394 -[wd: 4.00e-02] [lr: 1.62e-05][max mem allocated: 5.61e+04] (5666.0 ms)
INFO:root:[13,   512/ 1053] - train_loss: 0.2392 -[wd: 4.00e-02] [lr: 1.63e-05][max mem allocated: 5.61e+04] (5670.5 ms)
INFO:root:[13,   640/ 1053] - train_loss: 0.2390 -[wd: 4.00e-02] [lr: 1.63e-05][max mem allocated: 5.61e+04] (5670.1 ms)
INFO:root:[13,   768/ 1053] - train_loss: 0.2388 -[wd: 4.00e-02] [lr: 1.64e-05][max mem allocated: 5.61e+04] (5675.0 ms)
INFO:root:[13,   896/ 1053] - train_loss: 0.2386 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 5.61e+04] (5677.8 ms)
INFO:root:[13,  1024/ 1053] - train_loss: 0.2385 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 5.61e+04] (5677.7 ms)
INFO:root:Loss 0.2368
INFO:root:Epoch 14
INFO:root:[14,     0/ 1053] - train_loss: 0.2368 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 5.61e+04] (5330.6 ms)
INFO:root:[14,   128/ 1053] - train_loss: 0.2369 -[wd: 4.00e-02] [lr: 1.66e-05][max mem allocated: 5.61e+04] (5641.9 ms)
INFO:root:[14,   256/ 1053] - train_loss: 0.2368 -[wd: 4.00e-02] [lr: 1.66e-05][max mem allocated: 5.61e+04] (5656.7 ms)
INFO:root:[14,   384/ 1053] - train_loss: 0.2366 -[wd: 4.00e-02] [lr: 1.67e-05][max mem allocated: 5.61e+04] (5685.0 ms)
INFO:root:[14,   512/ 1053] - train_loss: 0.2365 -[wd: 4.00e-02] [lr: 1.68e-05][max mem allocated: 5.61e+04] (5694.5 ms)
INFO:root:[14,   640/ 1053] - train_loss: 0.2364 -[wd: 4.00e-02] [lr: 1.68e-05][max mem allocated: 5.61e+04] (5697.9 ms)
INFO:root:[14,   768/ 1053] - train_loss: 0.2363 -[wd: 4.00e-02] [lr: 1.69e-05][max mem allocated: 5.61e+04] (5701.5 ms)
INFO:root:[14,   896/ 1053] - train_loss: 0.2361 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 5.61e+04] (5707.8 ms)
INFO:root:[14,  1024/ 1053] - train_loss: 0.2360 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 5.61e+04] (5707.2 ms)
INFO:root:Loss 0.2348
INFO:root:Epoch 15
INFO:root:[15,     0/ 1053] - train_loss: 0.2350 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 5.61e+04] (5457.8 ms)
INFO:root:[15,   128/ 1053] - train_loss: 0.2350 -[wd: 4.00e-02] [lr: 1.71e-05][max mem allocated: 5.61e+04] (5660.5 ms)
INFO:root:[15,   256/ 1053] - train_loss: 0.2349 -[wd: 4.00e-02] [lr: 1.71e-05][max mem allocated: 5.61e+04] (5683.7 ms)
