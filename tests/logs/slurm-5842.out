INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:timm.models._helpers:Loaded state_dict_ema from checkpoint '/home/rtcalumby/adam/luciano/PlantCLEF2025/PlantCLEF2025/pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar'
Loading Vision Transformer: PilotVisionTransformer(
  (patch_embed): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (patch_drop): Identity()
    (norm_pre): Identity()
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (fc_norm): Identity()
    (head_drop): Dropout(p=0.0, inplace=False)
    (head): Identity()
  )
  (linear): Linear(in_features=1024, out_features=7806, bias=True)
  (blocks): ModuleList(
    (0-5): 6 x Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Loading state_dict: <All keys matched successfully>
img tensor: torch.Size([1024, 3, 518, 518])
selected_crops tensor([[[[-0.6452, -0.7993, -0.7650,  ..., -0.1143,  1.4098,  1.3755],
          [-0.6965, -0.4226, -0.7479,  ...,  0.1426,  1.3070,  1.2557],
          [ 0.4508,  0.5022, -0.6452,  ...,  0.2453,  0.9646,  1.2043],
          ...,
          [-0.3369,  0.0741,  1.2385,  ..., -0.9020, -0.9534, -0.8678],
          [-0.4226,  0.0227,  0.3994,  ..., -0.9705, -1.0562, -0.6623],
          [-0.2684, -0.3712, -0.8507,  ..., -0.0629, -0.3198, -0.3198]],

         [[-0.6176, -0.7402, -0.7227,  ..., -0.1275,  1.4307,  1.3782],
          [-0.6527, -0.3550, -0.6877,  ...,  0.1352,  1.3256,  1.2381],
          [ 0.4853,  0.5903, -0.5826,  ...,  0.2402,  0.9755,  1.2206],
          ...,
          [-0.3550,  0.0651,  1.2381,  ..., -0.9678, -1.0203, -0.9328],
          [-0.4426, -0.0049,  0.3803,  ..., -1.0378, -1.1253, -0.7227],
          [-0.2850, -0.3901, -0.8978,  ..., -0.1099, -0.3725, -0.3725]],

         [[-0.5844, -0.6890, -0.6367,  ..., -0.1312,  1.4200,  1.3677],
          [-0.6193, -0.3230, -0.6193,  ...,  0.1302,  1.3154,  1.2457],
          [ 0.5311,  0.6182, -0.5495,  ...,  0.2348,  0.9668,  1.1759],
          ...,
          [-0.3404,  0.1128,  1.3154,  ..., -0.9504, -1.0027, -0.9156],
          [-0.4275,  0.0431,  0.4614,  ..., -1.0201, -1.1073, -0.7064],
          [-0.2707, -0.3578, -0.7936,  ..., -0.0964, -0.3578, -0.3578]]],


        [[[-1.2445, -1.5185, -0.6965,  ..., -0.2513, -0.1657, -0.2342],
          [-1.0562, -1.7925, -1.4500,  ...,  0.0227,  0.0741, -0.4911],
          [-0.2856, -1.2788, -0.8849,  ...,  0.2282,  0.4508, -0.3027],
          ...,
          [-0.2856, -0.8164, -0.7993,  ..., -0.8164, -1.0390, -0.9705],
          [-1.3130, -1.3473, -0.8164,  ...,  0.8961,  0.1254, -0.4568],
          [-1.3987, -0.8507,  0.0227,  ...,  0.7762,  0.4851, -0.1657]],

         [[-1.3354, -1.6506, -0.8277,  ..., -0.2850, -0.2150, -0.2850],
          [-1.1604, -1.9132, -1.5630,  ..., -0.0224,  0.0301, -0.5476],
          [-0.4076, -1.4055, -0.9853,  ...,  0.2052,  0.4153, -0.3550],
          ...,
          [-0.3025, -0.8452, -0.8277,  ..., -0.7927, -1.0903, -1.0203],
          [-1.3704, -1.4055, -0.8452,  ...,  0.9055,  0.0476, -0.5476],
          [-1.4580, -0.8978,  0.0126,  ...,  0.7129,  0.3627, -0.3025]],

         [[-1.2293, -1.5953, -0.7587,  ..., -0.3230, -0.2010, -0.2707],
          [-1.1596, -1.8044, -1.5430,  ..., -0.0441,  0.0431, -0.5321],
          [-0.3753, -1.3687, -0.9853,  ...,  0.1651,  0.4265, -0.3404],
          ...,
          [-0.2532, -0.8110, -0.7936,  ..., -0.7761, -1.0376, -0.9853],
          [-1.2990, -1.3513, -0.8110,  ...,  0.9145,  0.0779, -0.4973],
          [-1.3861, -0.8284,  0.0431,  ...,  0.7751,  0.3916, -0.2358]]],


        [[[ 1.9920,  1.9920,  2.1462,  ..., -0.0972, -0.0801, -0.1486],
          [ 2.0434,  2.0777,  2.1290,  ..., -0.1657, -0.1486, -0.1143],
          [ 2.0605,  2.0263,  1.9578,  ..., -0.1143, -0.1486, -0.0972],
          ...,
          [ 2.0948,  2.1119,  2.1290,  ...,  0.3138, -0.3883, -0.4397],
          [ 2.0434,  2.0605,  2.1119,  ...,  0.2624, -0.4911, -0.9192],
          [ 2.0777,  2.1119,  2.1804,  ...,  0.9132,  0.0741, -0.7993]],

         [[ 1.9384,  1.9384,  2.0959,  ..., -0.3025, -0.2850, -0.3550],
          [ 1.9909,  2.0259,  2.0784,  ..., -0.3725, -0.3550, -0.3200],
          [ 2.0084,  1.9734,  1.9034,  ..., -0.3550, -0.3901, -0.3550],
          ...,
          [ 2.0609,  2.0784,  2.0959,  ...,  0.2752, -0.4426, -0.4951],
          [ 2.0084,  2.0259,  2.0784,  ...,  0.2227, -0.5476, -0.9853],
          [ 2.0434,  2.0784,  2.1485,  ...,  0.8880,  0.0301, -0.8627]],

         [[ 1.8557,  1.8557,  2.0125,  ..., -0.3753, -0.3404, -0.4101],
          [ 1.9080,  1.9428,  1.9951,  ..., -0.4101, -0.3927, -0.3578],
          [ 1.9254,  1.8905,  1.8208,  ..., -0.3753, -0.4275, -0.3753],
          ...,
          [ 1.9951,  2.0474,  2.0648,  ...,  0.2871, -0.4275, -0.4450],
          [ 1.9428,  1.9951,  2.0474,  ...,  0.2348, -0.4973, -0.9330],
          [ 1.9777,  2.0474,  2.1171,  ...,  0.9319,  0.0779, -0.8110]]],


        ...,


        [[[-0.4226, -0.7137, -0.5253,  ..., -0.8164, -1.6213, -1.1247],
          [-0.9020, -0.7308, -0.2513,  ..., -0.4226, -1.8268, -1.7925],
          [-1.0390, -0.3712,  0.0227,  ..., -0.3541, -0.8678, -1.5185],
          ...,
          [-0.3883,  0.0056,  0.2282,  ..., -0.9705, -1.0733, -1.0904],
          [-1.3302, -0.7822, -0.0458,  ..., -1.0733, -1.0733, -1.1247],
          [-1.4843, -1.1932, -0.4054,  ..., -1.1247, -1.1760, -1.1075]],

         [[-0.4776, -0.7752, -0.5826,  ..., -0.8803, -1.6681, -1.1604],
          [-0.9678, -0.7927, -0.3025,  ..., -0.4776, -1.8782, -1.8431],
          [-1.1078, -0.4251, -0.0224,  ..., -0.4076, -0.8978, -1.5630],
          ...,
          [-0.3375,  0.0301,  0.2402,  ..., -0.6001, -0.6527, -0.6702],
          [-1.3004, -0.7752, -0.0399,  ..., -0.6877, -0.6527, -0.7052],
          [-1.4755, -1.1604, -0.4076,  ..., -0.7402, -0.7577, -0.6877]],

         [[-0.4275, -0.7238, -0.5321,  ..., -0.8458, -1.5081, -1.1073],
          [-0.9156, -0.7413, -0.2532,  ..., -0.4450, -1.7696, -1.8044],
          [-1.0550, -0.3753,  0.0256,  ..., -0.3927, -0.8633, -1.5256],
          ...,
          [-0.3230,  0.0431,  0.2696,  ..., -0.7761, -0.8458, -0.8633],
          [-1.2816, -0.7587, -0.0092,  ..., -0.8807, -0.8458, -0.8981],
          [-1.4384, -1.1421, -0.3927,  ..., -0.9156, -0.9504, -0.8807]]],


        [[[ 0.2282, -0.1486, -1.1075,  ..., -0.8678, -0.9192, -0.7822],
          [-1.3130, -0.6623, -0.8678,  ..., -0.6623, -0.7137, -0.7650],
          [-1.7069, -0.9363, -0.5253,  ..., -0.7479, -0.6623, -0.7479],
          ...,
          [-1.0562, -1.0390, -0.9877,  ..., -1.0048, -0.9877, -0.9363],
          [-1.0562, -1.0048, -0.9020,  ..., -0.7308, -0.6965, -0.5596],
          [-1.0219, -1.0048, -0.7479,  ..., -0.4911, -0.5767, -0.5082]],

         [[ 0.2227, -0.1625, -1.1429,  ..., -0.5826, -0.6001, -0.5126],
          [-1.3529, -0.6877, -0.8978,  ..., -0.3725, -0.4251, -0.4951],
          [-1.7556, -0.9678, -0.5476,  ..., -0.4601, -0.3725, -0.4776],
          ...,
          [-0.6877, -0.6702, -0.6352,  ..., -0.6877, -0.6702, -0.6527],
          [-0.6877, -0.6352, -0.5476,  ..., -0.3901, -0.3550, -0.2500],
          [-0.6352, -0.6352, -0.3901,  ..., -0.1099, -0.1975, -0.1800]],

         [[ 0.2522, -0.1312, -1.1073,  ..., -0.7761, -0.7936, -0.6541],
          [-1.3164, -0.6541, -0.8633,  ..., -0.5670, -0.6193, -0.6367],
          [-1.7173, -0.9330, -0.5147,  ..., -0.6541, -0.5670, -0.6193],
          ...,
          [-0.8633, -0.8458, -0.8110,  ..., -1.1247, -1.1073, -1.0550],
          [-0.8633, -0.8110, -0.7238,  ..., -0.8633, -0.8458, -0.7064],
          [-0.8110, -0.8110, -0.5670,  ..., -0.6018, -0.6890, -0.6541]]],


        [[[-0.8335, -0.9020, -1.0390,  ..., -0.1999, -0.3883, -0.5424],
          [-0.9192, -0.7650, -0.7993,  ..., -0.3027, -0.2342, -0.2513],
          [-0.8849, -0.8678, -0.6452,  ..., -0.1999,  0.0398, -0.0801],
          ...,
          [-0.8678, -0.7993, -0.7479,  ..., -0.6623, -1.0562, -0.5253],
          [-0.5424, -0.5253, -0.5596,  ..., -0.8164, -0.9020, -0.4911],
          [-0.5424, -0.5938, -0.6281,  ..., -1.0048, -0.8849, -0.5596]],

         [[-0.5826, -0.6527, -0.8627,  ..., -0.2500, -0.4426, -0.6001],
          [-0.6702, -0.5126, -0.5826,  ..., -0.3550, -0.2850, -0.3025],
          [-0.6352, -0.6176, -0.3901,  ..., -0.2500, -0.0049, -0.1275],
          ...,
          [-0.6001, -0.5126, -0.4601,  ..., -0.5301, -0.9328, -0.3901],
          [-0.2325, -0.2150, -0.2500,  ..., -0.7577, -0.8452, -0.4251],
          [-0.2325, -0.2675, -0.3025,  ..., -1.0203, -0.8978, -0.5651]],

         [[-0.7238, -0.7936, -0.9330,  ..., -0.2358, -0.4275, -0.5844],
          [-0.8110, -0.6541, -0.6890,  ..., -0.3404, -0.2707, -0.2881],
          [-0.7761, -0.7587, -0.5321,  ..., -0.2358,  0.0082, -0.1138],
          ...,
          [-1.0027, -0.9330, -0.8807,  ..., -0.7413, -1.1421, -0.6018],
          [-0.6715, -0.6715, -0.6890,  ..., -0.8981, -0.9853, -0.5670],
          [-0.7064, -0.7587, -0.7761,  ..., -1.0724, -0.9678, -0.6367]]]])
