INFO:root:called-params config/configs.yaml
INFO:root:loaded params...
INFO:root:{'experiment_code': 0, 'k_means': {'K': 7806}, 'patches': {'N': 64}, 'num_workers': 16, 'base_dir': '/home/rtcalumby/adam/luciano/PlantCLEF2025/PlantCLEF2025/', 'pretrained_path': 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar', 'data': {'class_mapping': 'pretrained_models/class_mapping.txt', 'species_mapping': 'pretrained_models/species_id_to_name.txt', 'test_data': '/home/rtcalumby/adam/luciano/PlantCLEF2025/test_dataset/'}, 'batch_size': 1, 'gradient_accumulation': 128, 'optimization': {'epochs': 150, 'final_lr': 1e-06, 'final_weight_decay': 0.4, 'ipe_scale': 1.0, 'lr': 0.001, 'start_lr': 1e-05, 'warmup': 15, 'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/2)
INFO:root:Initialized (rank/world-size) 0/2
INFO:timm.models._helpers:Loaded state_dict_ema from checkpoint 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar'
Test dataset created for rank 0 world_size:  2
Test dataset, length: 1053
INFO:root:Loading Vision Transformer: VisionTransformer(
  (patch_embed): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (patch_drop): Identity()
    (norm_pre): Identity()
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (fc_norm): Identity()
    (head_drop): Dropout(p=0.0, inplace=False)
    (head): Identity()
  )
  (linear): Linear(in_features=1024, out_features=7806, bias=True)
  (blocks): ModuleList(
    (0-5): 6 x Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
INFO:root:Using AdamW
Rank 1
Test dataset created for rank 1 world_size:  2
Test dataset, length: 1053
Allocated Memory with model loading: 1.0259356498718262  GB
Allocated Memory with model loading: 1.0259356498718262  GB
INFO:root:Epoch 1
INFO:root:[1,     0/ 1053] - train_loss: 0.6439 -[wd: 1.00e-02] [lr: 1.00e-03][max mem allocated: 3.74e+04] (5316.9 ms)
INFO:root:[1,   128/ 1053] - train_loss: 0.6433 -[wd: 4.00e-02] [lr: 1.01e-05][max mem allocated: 3.78e+04] (3242.8 ms)
INFO:root:[1,   256/ 1053] - train_loss: 0.6397 -[wd: 4.00e-02] [lr: 1.01e-05][max mem allocated: 3.80e+04] (3238.4 ms)
INFO:root:[1,   384/ 1053] - train_loss: 0.6362 -[wd: 4.00e-02] [lr: 1.02e-05][max mem allocated: 3.80e+04] (3236.6 ms)
INFO:root:[1,   512/ 1053] - train_loss: 0.6326 -[wd: 4.00e-02] [lr: 1.03e-05][max mem allocated: 3.80e+04] (3236.2 ms)
INFO:root:[1,   640/ 1053] - train_loss: 0.6290 -[wd: 4.00e-02] [lr: 1.03e-05][max mem allocated: 3.80e+04] (3234.9 ms)
INFO:root:[1,   768/ 1053] - train_loss: 0.6253 -[wd: 4.00e-02] [lr: 1.04e-05][max mem allocated: 3.80e+04] (3234.0 ms)
INFO:root:[1,   896/ 1053] - train_loss: 0.6215 -[wd: 4.00e-02] [lr: 1.04e-05][max mem allocated: 3.80e+04] (3234.2 ms)
INFO:root:[1,  1024/ 1053] - train_loss: 0.6175 -[wd: 4.00e-02] [lr: 1.05e-05][max mem allocated: 3.80e+04] (3234.1 ms)
INFO:root:Loss 0.5837
INFO:root:Epoch 2
INFO:root:[2,     0/ 1053] - train_loss: 0.5796 -[wd: 4.00e-02] [lr: 1.05e-05][max mem allocated: 3.80e+04] (3142.8 ms)
INFO:root:[2,   128/ 1053] - train_loss: 0.5816 -[wd: 4.00e-02] [lr: 1.06e-05][max mem allocated: 3.80e+04] (3230.8 ms)
INFO:root:[2,   256/ 1053] - train_loss: 0.5773 -[wd: 4.00e-02] [lr: 1.06e-05][max mem allocated: 3.80e+04] (3229.9 ms)
INFO:root:[2,   384/ 1053] - train_loss: 0.5729 -[wd: 4.00e-02] [lr: 1.07e-05][max mem allocated: 3.80e+04] (3231.0 ms)
INFO:root:[2,   512/ 1053] - train_loss: 0.5684 -[wd: 4.00e-02] [lr: 1.08e-05][max mem allocated: 3.80e+04] (3230.8 ms)
INFO:root:[2,   640/ 1053] - train_loss: 0.5639 -[wd: 4.00e-02] [lr: 1.08e-05][max mem allocated: 3.80e+04] (3231.0 ms)
INFO:root:[2,   768/ 1053] - train_loss: 0.5594 -[wd: 4.00e-02] [lr: 1.09e-05][max mem allocated: 3.80e+04] (3231.5 ms)
INFO:root:[2,   896/ 1053] - train_loss: 0.5549 -[wd: 4.00e-02] [lr: 1.09e-05][max mem allocated: 3.80e+04] (3231.7 ms)
INFO:root:[2,  1024/ 1053] - train_loss: 0.5504 -[wd: 4.00e-02] [lr: 1.10e-05][max mem allocated: 3.80e+04] (3232.3 ms)
INFO:root:Loss 0.5125
INFO:root:Epoch 3
INFO:root:[3,     0/ 1053] - train_loss: 0.5105 -[wd: 4.00e-02] [lr: 1.10e-05][max mem allocated: 3.80e+04] (3228.6 ms)
INFO:root:[3,   128/ 1053] - train_loss: 0.5097 -[wd: 4.00e-02] [lr: 1.11e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[3,   256/ 1053] - train_loss: 0.5054 -[wd: 4.00e-02] [lr: 1.11e-05][max mem allocated: 3.80e+04] (3235.5 ms)
INFO:root:[3,   384/ 1053] - train_loss: 0.5010 -[wd: 4.00e-02] [lr: 1.12e-05][max mem allocated: 3.80e+04] (3235.9 ms)
INFO:root:[3,   512/ 1053] - train_loss: 0.4966 -[wd: 4.00e-02] [lr: 1.13e-05][max mem allocated: 3.80e+04] (3235.0 ms)
INFO:root:[3,   640/ 1053] - train_loss: 0.4923 -[wd: 4.00e-02] [lr: 1.13e-05][max mem allocated: 3.80e+04] (3234.0 ms)
INFO:root:[3,   768/ 1053] - train_loss: 0.4882 -[wd: 4.00e-02] [lr: 1.14e-05][max mem allocated: 3.80e+04] (3234.3 ms)
INFO:root:[3,   896/ 1053] - train_loss: 0.4840 -[wd: 4.00e-02] [lr: 1.14e-05][max mem allocated: 3.80e+04] (3234.1 ms)
INFO:root:[3,  1024/ 1053] - train_loss: 0.4799 -[wd: 4.00e-02] [lr: 1.15e-05][max mem allocated: 3.80e+04] (3234.7 ms)
INFO:root:Loss 0.4432
INFO:root:Epoch 4
INFO:root:[4,     0/ 1053] - train_loss: 0.4444 -[wd: 4.00e-02] [lr: 1.15e-05][max mem allocated: 3.80e+04] (3100.9 ms)
INFO:root:[4,   128/ 1053] - train_loss: 0.4437 -[wd: 4.00e-02] [lr: 1.16e-05][max mem allocated: 3.80e+04] (3232.8 ms)
INFO:root:[4,   256/ 1053] - train_loss: 0.4400 -[wd: 4.00e-02] [lr: 1.16e-05][max mem allocated: 3.80e+04] (3232.2 ms)
INFO:root:[4,   384/ 1053] - train_loss: 0.4363 -[wd: 4.00e-02] [lr: 1.17e-05][max mem allocated: 3.80e+04] (3230.7 ms)
INFO:root:[4,   512/ 1053] - train_loss: 0.4328 -[wd: 4.00e-02] [lr: 1.18e-05][max mem allocated: 3.80e+04] (3230.9 ms)
INFO:root:[4,   640/ 1053] - train_loss: 0.4293 -[wd: 4.00e-02] [lr: 1.18e-05][max mem allocated: 3.80e+04] (3231.5 ms)
INFO:root:[4,   768/ 1053] - train_loss: 0.4258 -[wd: 4.00e-02] [lr: 1.19e-05][max mem allocated: 3.80e+04] (3231.8 ms)
INFO:root:[4,   896/ 1053] - train_loss: 0.4224 -[wd: 4.00e-02] [lr: 1.19e-05][max mem allocated: 3.80e+04] (3232.8 ms)
INFO:root:[4,  1024/ 1053] - train_loss: 0.4191 -[wd: 4.00e-02] [lr: 1.20e-05][max mem allocated: 3.80e+04] (3233.4 ms)
INFO:root:Loss 0.3900
INFO:root:Epoch 5
INFO:root:[5,     0/ 1053] - train_loss: 0.3894 -[wd: 4.00e-02] [lr: 1.20e-05][max mem allocated: 3.80e+04] (3095.7 ms)
INFO:root:[5,   128/ 1053] - train_loss: 0.3899 -[wd: 4.00e-02] [lr: 1.21e-05][max mem allocated: 3.80e+04] (3232.8 ms)
INFO:root:[5,   256/ 1053] - train_loss: 0.3870 -[wd: 4.00e-02] [lr: 1.21e-05][max mem allocated: 3.80e+04] (3234.8 ms)
INFO:root:[5,   384/ 1053] - train_loss: 0.3841 -[wd: 4.00e-02] [lr: 1.22e-05][max mem allocated: 3.80e+04] (3235.3 ms)
INFO:root:[5,   512/ 1053] - train_loss: 0.3813 -[wd: 4.00e-02] [lr: 1.23e-05][max mem allocated: 3.80e+04] (3235.2 ms)
INFO:root:[5,   640/ 1053] - train_loss: 0.3786 -[wd: 4.00e-02] [lr: 1.23e-05][max mem allocated: 3.80e+04] (3235.6 ms)
INFO:root:[5,   768/ 1053] - train_loss: 0.3759 -[wd: 4.00e-02] [lr: 1.24e-05][max mem allocated: 3.80e+04] (3234.9 ms)
INFO:root:[5,   896/ 1053] - train_loss: 0.3732 -[wd: 4.00e-02] [lr: 1.24e-05][max mem allocated: 3.80e+04] (3234.8 ms)
INFO:root:[5,  1024/ 1053] - train_loss: 0.3707 -[wd: 4.00e-02] [lr: 1.25e-05][max mem allocated: 3.80e+04] (3235.3 ms)
INFO:root:Loss 0.3476
INFO:root:Epoch 6
INFO:root:[6,     0/ 1053] - train_loss: 0.3474 -[wd: 4.00e-02] [lr: 1.25e-05][max mem allocated: 3.80e+04] (2932.0 ms)
INFO:root:[6,   128/ 1053] - train_loss: 0.3481 -[wd: 4.00e-02] [lr: 1.26e-05][max mem allocated: 3.80e+04] (3228.2 ms)
INFO:root:[6,   256/ 1053] - train_loss: 0.3458 -[wd: 4.00e-02] [lr: 1.26e-05][max mem allocated: 3.80e+04] (3230.1 ms)
INFO:root:[6,   384/ 1053] - train_loss: 0.3436 -[wd: 4.00e-02] [lr: 1.27e-05][max mem allocated: 3.80e+04] (3231.3 ms)
INFO:root:[6,   512/ 1053] - train_loss: 0.3415 -[wd: 4.00e-02] [lr: 1.28e-05][max mem allocated: 3.80e+04] (3231.6 ms)
INFO:root:[6,   640/ 1053] - train_loss: 0.3394 -[wd: 4.00e-02] [lr: 1.28e-05][max mem allocated: 3.80e+04] (3231.8 ms)
INFO:root:[6,   768/ 1053] - train_loss: 0.3373 -[wd: 4.00e-02] [lr: 1.29e-05][max mem allocated: 3.80e+04] (3231.4 ms)
INFO:root:[6,   896/ 1053] - train_loss: 0.3353 -[wd: 4.00e-02] [lr: 1.29e-05][max mem allocated: 3.80e+04] (3232.2 ms)
INFO:root:[6,  1024/ 1053] - train_loss: 0.3334 -[wd: 4.00e-02] [lr: 1.30e-05][max mem allocated: 3.80e+04] (3232.4 ms)
INFO:root:Loss 0.3159
INFO:root:Epoch 7
INFO:root:[7,     0/ 1053] - train_loss: 0.3158 -[wd: 4.00e-02] [lr: 1.30e-05][max mem allocated: 3.80e+04] (3032.3 ms)
INFO:root:[7,   128/ 1053] - train_loss: 0.3163 -[wd: 4.00e-02] [lr: 1.31e-05][max mem allocated: 3.80e+04] (3233.9 ms)
INFO:root:[7,   256/ 1053] - train_loss: 0.3146 -[wd: 4.00e-02] [lr: 1.31e-05][max mem allocated: 3.80e+04] (3235.6 ms)
INFO:root:[7,   384/ 1053] - train_loss: 0.3130 -[wd: 4.00e-02] [lr: 1.32e-05][max mem allocated: 3.80e+04] (3236.5 ms)
INFO:root:[7,   512/ 1053] - train_loss: 0.3114 -[wd: 4.00e-02] [lr: 1.33e-05][max mem allocated: 3.80e+04] (3235.8 ms)
INFO:root:[7,   640/ 1053] - train_loss: 0.3098 -[wd: 4.00e-02] [lr: 1.33e-05][max mem allocated: 3.80e+04] (3236.4 ms)
INFO:root:[7,   768/ 1053] - train_loss: 0.3083 -[wd: 4.00e-02] [lr: 1.34e-05][max mem allocated: 3.80e+04] (3236.8 ms)
INFO:root:[7,   896/ 1053] - train_loss: 0.3068 -[wd: 4.00e-02] [lr: 1.34e-05][max mem allocated: 3.80e+04] (3236.7 ms)
INFO:root:[7,  1024/ 1053] - train_loss: 0.3053 -[wd: 4.00e-02] [lr: 1.35e-05][max mem allocated: 3.80e+04] (3236.3 ms)
INFO:root:Loss 0.2922
INFO:root:Epoch 8
INFO:root:[8,     0/ 1053] - train_loss: 0.2923 -[wd: 4.00e-02] [lr: 1.35e-05][max mem allocated: 3.80e+04] (2938.4 ms)
INFO:root:[8,   128/ 1053] - train_loss: 0.2926 -[wd: 4.00e-02] [lr: 1.36e-05][max mem allocated: 3.80e+04] (3230.8 ms)
INFO:root:[8,   256/ 1053] - train_loss: 0.2913 -[wd: 4.00e-02] [lr: 1.36e-05][max mem allocated: 3.80e+04] (3231.1 ms)
INFO:root:[8,   384/ 1053] - train_loss: 0.2901 -[wd: 4.00e-02] [lr: 1.37e-05][max mem allocated: 3.80e+04] (3232.1 ms)
INFO:root:[8,   512/ 1053] - train_loss: 0.2889 -[wd: 4.00e-02] [lr: 1.38e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[8,   640/ 1053] - train_loss: 0.2877 -[wd: 4.00e-02] [lr: 1.38e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[8,   768/ 1053] - train_loss: 0.2866 -[wd: 4.00e-02] [lr: 1.39e-05][max mem allocated: 3.80e+04] (3233.4 ms)
INFO:root:[8,   896/ 1053] - train_loss: 0.2855 -[wd: 4.00e-02] [lr: 1.39e-05][max mem allocated: 3.80e+04] (3233.5 ms)
INFO:root:[8,  1024/ 1053] - train_loss: 0.2844 -[wd: 4.00e-02] [lr: 1.40e-05][max mem allocated: 3.80e+04] (3233.9 ms)
INFO:root:Loss 0.2749
INFO:root:Epoch 9
INFO:root:[9,     0/ 1053] - train_loss: 0.2749 -[wd: 4.00e-02] [lr: 1.40e-05][max mem allocated: 3.80e+04] (3011.3 ms)
INFO:root:[9,   128/ 1053] - train_loss: 0.2750 -[wd: 4.00e-02] [lr: 1.41e-05][max mem allocated: 3.80e+04] (3235.2 ms)
INFO:root:[9,   256/ 1053] - train_loss: 0.2741 -[wd: 4.00e-02] [lr: 1.41e-05][max mem allocated: 3.80e+04] (3235.4 ms)
INFO:root:[9,   384/ 1053] - train_loss: 0.2732 -[wd: 4.00e-02] [lr: 1.42e-05][max mem allocated: 3.80e+04] (3235.1 ms)
INFO:root:[9,   512/ 1053] - train_loss: 0.2723 -[wd: 4.00e-02] [lr: 1.43e-05][max mem allocated: 3.80e+04] (3234.6 ms)
INFO:root:[9,   640/ 1053] - train_loss: 0.2715 -[wd: 4.00e-02] [lr: 1.43e-05][max mem allocated: 3.80e+04] (3235.0 ms)
INFO:root:[9,   768/ 1053] - train_loss: 0.2707 -[wd: 4.00e-02] [lr: 1.44e-05][max mem allocated: 3.80e+04] (3234.5 ms)
INFO:root:[9,   896/ 1053] - train_loss: 0.2699 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 3.80e+04] (3234.3 ms)
INFO:root:[9,  1024/ 1053] - train_loss: 0.2691 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 3.80e+04] (3235.0 ms)
INFO:root:Loss 0.2623
INFO:root:Epoch 10
INFO:root:[10,     0/ 1053] - train_loss: 0.2624 -[wd: 4.00e-02] [lr: 1.45e-05][max mem allocated: 3.80e+04] (2909.1 ms)
INFO:root:[10,   128/ 1053] - train_loss: 0.2622 -[wd: 4.00e-02] [lr: 1.46e-05][max mem allocated: 3.80e+04] (3229.4 ms)
INFO:root:[10,   256/ 1053] - train_loss: 0.2615 -[wd: 4.00e-02] [lr: 1.46e-05][max mem allocated: 3.80e+04] (3228.5 ms)
INFO:root:[10,   384/ 1053] - train_loss: 0.2609 -[wd: 4.00e-02] [lr: 1.47e-05][max mem allocated: 3.80e+04] (3231.0 ms)
INFO:root:[10,   512/ 1053] - train_loss: 0.2603 -[wd: 4.00e-02] [lr: 1.48e-05][max mem allocated: 3.80e+04] (3231.4 ms)
INFO:root:[10,   640/ 1053] - train_loss: 0.2596 -[wd: 4.00e-02] [lr: 1.48e-05][max mem allocated: 3.80e+04] (3231.2 ms)
INFO:root:[10,   768/ 1053] - train_loss: 0.2590 -[wd: 4.00e-02] [lr: 1.49e-05][max mem allocated: 3.80e+04] (3232.0 ms)
INFO:root:[10,   896/ 1053] - train_loss: 0.2584 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 3.80e+04] (3232.2 ms)
INFO:root:[10,  1024/ 1053] - train_loss: 0.2579 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 3.80e+04] (3232.1 ms)
INFO:root:Loss 0.2531
INFO:root:Epoch 11
INFO:root:[11,     0/ 1053] - train_loss: 0.2530 -[wd: 4.00e-02] [lr: 1.50e-05][max mem allocated: 3.80e+04] (3101.0 ms)
INFO:root:[11,   128/ 1053] - train_loss: 0.2529 -[wd: 4.00e-02] [lr: 1.51e-05][max mem allocated: 3.80e+04] (3233.4 ms)
INFO:root:[11,   256/ 1053] - train_loss: 0.2524 -[wd: 4.00e-02] [lr: 1.51e-05][max mem allocated: 3.80e+04] (3234.3 ms)
INFO:root:[11,   384/ 1053] - train_loss: 0.2519 -[wd: 4.00e-02] [lr: 1.52e-05][max mem allocated: 3.80e+04] (3233.9 ms)
INFO:root:[11,   512/ 1053] - train_loss: 0.2515 -[wd: 4.00e-02] [lr: 1.53e-05][max mem allocated: 3.80e+04] (3234.9 ms)
INFO:root:[11,   640/ 1053] - train_loss: 0.2510 -[wd: 4.00e-02] [lr: 1.53e-05][max mem allocated: 3.80e+04] (3234.6 ms)
INFO:root:[11,   768/ 1053] - train_loss: 0.2506 -[wd: 4.00e-02] [lr: 1.54e-05][max mem allocated: 3.80e+04] (3234.6 ms)
INFO:root:[11,   896/ 1053] - train_loss: 0.2501 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 3.80e+04] (3235.2 ms)
INFO:root:[11,  1024/ 1053] - train_loss: 0.2497 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 3.80e+04] (3235.2 ms)
INFO:root:Loss 0.2460
INFO:root:Epoch 12
INFO:root:[12,     0/ 1053] - train_loss: 0.2463 -[wd: 4.00e-02] [lr: 1.55e-05][max mem allocated: 3.80e+04] (3120.0 ms)
INFO:root:[12,   128/ 1053] - train_loss: 0.2461 -[wd: 4.00e-02] [lr: 1.56e-05][max mem allocated: 3.80e+04] (3234.2 ms)
INFO:root:[12,   256/ 1053] - train_loss: 0.2458 -[wd: 4.00e-02] [lr: 1.56e-05][max mem allocated: 3.80e+04] (3231.3 ms)
INFO:root:[12,   384/ 1053] - train_loss: 0.2454 -[wd: 4.00e-02] [lr: 1.57e-05][max mem allocated: 3.80e+04] (3231.6 ms)
INFO:root:[12,   512/ 1053] - train_loss: 0.2451 -[wd: 4.00e-02] [lr: 1.58e-05][max mem allocated: 3.80e+04] (3231.6 ms)
INFO:root:[12,   640/ 1053] - train_loss: 0.2447 -[wd: 4.00e-02] [lr: 1.58e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[12,   768/ 1053] - train_loss: 0.2444 -[wd: 4.00e-02] [lr: 1.59e-05][max mem allocated: 3.80e+04] (3234.1 ms)
INFO:root:[12,   896/ 1053] - train_loss: 0.2441 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 3.80e+04] (3234.1 ms)
INFO:root:[12,  1024/ 1053] - train_loss: 0.2438 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 3.80e+04] (3234.3 ms)
INFO:root:Loss 0.2413
INFO:root:Epoch 13
INFO:root:[13,     0/ 1053] - train_loss: 0.2412 -[wd: 4.00e-02] [lr: 1.60e-05][max mem allocated: 3.80e+04] (3086.2 ms)
INFO:root:[13,   128/ 1053] - train_loss: 0.2411 -[wd: 4.00e-02] [lr: 1.61e-05][max mem allocated: 3.80e+04] (3232.3 ms)
INFO:root:[13,   256/ 1053] - train_loss: 0.2409 -[wd: 4.00e-02] [lr: 1.61e-05][max mem allocated: 3.80e+04] (3232.0 ms)
INFO:root:[13,   384/ 1053] - train_loss: 0.2406 -[wd: 4.00e-02] [lr: 1.62e-05][max mem allocated: 3.80e+04] (3231.6 ms)
INFO:root:[13,   512/ 1053] - train_loss: 0.2404 -[wd: 4.00e-02] [lr: 1.63e-05][max mem allocated: 3.80e+04] (3234.0 ms)
INFO:root:[13,   640/ 1053] - train_loss: 0.2402 -[wd: 4.00e-02] [lr: 1.63e-05][max mem allocated: 3.80e+04] (3234.0 ms)
INFO:root:[13,   768/ 1053] - train_loss: 0.2399 -[wd: 4.00e-02] [lr: 1.64e-05][max mem allocated: 3.80e+04] (3234.1 ms)
INFO:root:[13,   896/ 1053] - train_loss: 0.2397 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 3.80e+04] (3234.5 ms)
INFO:root:[13,  1024/ 1053] - train_loss: 0.2395 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 3.80e+04] (3234.8 ms)
INFO:root:Loss 0.2375
INFO:root:Epoch 14
INFO:root:[14,     0/ 1053] - train_loss: 0.2374 -[wd: 4.00e-02] [lr: 1.65e-05][max mem allocated: 3.80e+04] (3072.3 ms)
INFO:root:[14,   128/ 1053] - train_loss: 0.2375 -[wd: 4.00e-02] [lr: 1.66e-05][max mem allocated: 3.80e+04] (3230.4 ms)
INFO:root:[14,   256/ 1053] - train_loss: 0.2373 -[wd: 4.00e-02] [lr: 1.66e-05][max mem allocated: 3.80e+04] (3231.1 ms)
INFO:root:[14,   384/ 1053] - train_loss: 0.2371 -[wd: 4.00e-02] [lr: 1.67e-05][max mem allocated: 3.80e+04] (3234.3 ms)
INFO:root:[14,   512/ 1053] - train_loss: 0.2369 -[wd: 4.00e-02] [lr: 1.68e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[14,   640/ 1053] - train_loss: 0.2367 -[wd: 4.00e-02] [lr: 1.68e-05][max mem allocated: 3.80e+04] (3231.7 ms)
INFO:root:[14,   768/ 1053] - train_loss: 0.2365 -[wd: 4.00e-02] [lr: 1.69e-05][max mem allocated: 3.80e+04] (3231.5 ms)
INFO:root:[14,   896/ 1053] - train_loss: 0.2364 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 3.80e+04] (3231.4 ms)
INFO:root:[14,  1024/ 1053] - train_loss: 0.2362 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 3.80e+04] (3231.7 ms)
INFO:root:Loss 0.2346
INFO:root:Epoch 15
INFO:root:[15,     0/ 1053] - train_loss: 0.2347 -[wd: 4.00e-02] [lr: 1.70e-05][max mem allocated: 3.80e+04] (2919.7 ms)
INFO:root:[15,   128/ 1053] - train_loss: 0.2346 -[wd: 4.00e-02] [lr: 1.71e-05][max mem allocated: 3.80e+04] (3229.2 ms)
INFO:root:[15,   256/ 1053] - train_loss: 0.2345 -[wd: 4.00e-02] [lr: 1.71e-05][max mem allocated: 3.80e+04] (3230.1 ms)
INFO:root:[15,   384/ 1053] - train_loss: 0.2343 -[wd: 4.00e-02] [lr: 1.72e-05][max mem allocated: 3.80e+04] (3232.2 ms)
INFO:root:[15,   512/ 1053] - train_loss: 0.2342 -[wd: 4.00e-02] [lr: 1.73e-05][max mem allocated: 3.80e+04] (3231.2 ms)
INFO:root:[15,   640/ 1053] - train_loss: 0.2340 -[wd: 4.00e-02] [lr: 1.73e-05][max mem allocated: 3.80e+04] (3232.3 ms)
INFO:root:[15,   768/ 1053] - train_loss: 0.2339 -[wd: 4.00e-02] [lr: 1.74e-05][max mem allocated: 3.80e+04] (3233.3 ms)
INFO:root:[15,   896/ 1053] - train_loss: 0.2337 -[wd: 4.00e-02] [lr: 1.75e-05][max mem allocated: 3.80e+04] (3233.9 ms)
INFO:root:[15,  1024/ 1053] - train_loss: 0.2336 -[wd: 4.00e-02] [lr: 1.75e-05][max mem allocated: 3.80e+04] (3233.0 ms)
INFO:root:Loss 0.2323
INFO:root:Epoch 16
INFO:root:[16,     0/ 1053] - train_loss: 0.2323 -[wd: 4.00e-02] [lr: 1.75e-05][max mem allocated: 3.80e+04] (3004.4 ms)
INFO:root:[16,   128/ 1053] - train_loss: 0.2323 -[wd: 4.00e-02] [lr: 1.76e-05][max mem allocated: 3.80e+04] (3232.7 ms)
INFO:root:[16,   256/ 1053] - train_loss: 0.2321 -[wd: 4.00e-02] [lr: 1.76e-05][max mem allocated: 3.80e+04] (3233.5 ms)
INFO:root:[16,   384/ 1053] - train_loss: 0.2320 -[wd: 4.00e-02] [lr: 1.77e-05][max mem allocated: 3.80e+04] (3232.5 ms)
INFO:root:[16,   512/ 1053] - train_loss: 0.2319 -[wd: 4.00e-02] [lr: 1.78e-05][max mem allocated: 3.80e+04] (3232.5 ms)
INFO:root:[16,   640/ 1053] - train_loss: 0.2317 -[wd: 4.00e-02] [lr: 1.78e-05][max mem allocated: 3.80e+04] (3231.7 ms)
INFO:root:[16,   768/ 1053] - train_loss: 0.2316 -[wd: 4.00e-02] [lr: 1.79e-05][max mem allocated: 3.80e+04] (3231.3 ms)
INFO:root:[16,   896/ 1053] - train_loss: 0.2315 -[wd: 4.00e-02] [lr: 1.80e-05][max mem allocated: 3.80e+04] (3231.8 ms)
INFO:root:[16,  1024/ 1053] - train_loss: 0.2313 -[wd: 4.00e-02] [lr: 1.80e-05][max mem allocated: 3.80e+04] (3231.5 ms)
INFO:root:Loss 0.2301
INFO:root:Epoch 17
INFO:root:[17,     0/ 1053] - train_loss: 0.2302 -[wd: 4.00e-02] [lr: 1.80e-05][max mem allocated: 3.80e+04] (3289.9 ms)
INFO:root:[17,   128/ 1053] - train_loss: 0.2301 -[wd: 4.00e-02] [lr: 1.81e-05][max mem allocated: 3.80e+04] (3233.8 ms)
INFO:root:[17,   256/ 1053] - train_loss: 0.2300 -[wd: 4.00e-02] [lr: 1.81e-05][max mem allocated: 3.80e+04] (3232.9 ms)
INFO:root:[17,   384/ 1053] - train_loss: 0.2299 -[wd: 4.00e-02] [lr: 1.82e-05][max mem allocated: 3.80e+04] (3232.1 ms)
INFO:root:[17,   512/ 1053] - train_loss: 0.2297 -[wd: 4.00e-02] [lr: 1.83e-05][max mem allocated: 3.80e+04] (3233.4 ms)
INFO:root:[17,   640/ 1053] - train_loss: 0.2296 -[wd: 4.00e-02] [lr: 1.83e-05][max mem allocated: 3.80e+04] (3233.4 ms)
INFO:root:[17,   768/ 1053] - train_loss: 0.2295 -[wd: 4.00e-02] [lr: 1.84e-05][max mem allocated: 3.80e+04] (3233.5 ms)
INFO:root:[17,   896/ 1053] - train_loss: 0.2293 -[wd: 4.00e-02] [lr: 1.85e-05][max mem allocated: 3.80e+04] (3233.6 ms)
INFO:root:[17,  1024/ 1053] - train_loss: 0.2292 -[wd: 4.00e-02] [lr: 1.85e-05][max mem allocated: 3.80e+04] (3233.2 ms)
INFO:root:Loss 0.2279
INFO:root:Epoch 18
INFO:root:[18,     0/ 1053] - train_loss: 0.2280 -[wd: 4.00e-02] [lr: 1.85e-05][max mem allocated: 3.80e+04] (3363.9 ms)
INFO:root:[18,   128/ 1053] - train_loss: 0.2280 -[wd: 4.00e-02] [lr: 1.86e-05][max mem allocated: 3.80e+04] (3117.0 ms)
INFO:root:[18,   256/ 1053] - train_loss: 0.2278 -[wd: 4.00e-02] [lr: 1.86e-05][max mem allocated: 3.80e+04] (3116.6 ms)
INFO:root:[18,   384/ 1053] - train_loss: 0.2277 -[wd: 4.00e-02] [lr: 1.87e-05][max mem allocated: 3.80e+04] (3114.9 ms)
INFO:root:[18,   512/ 1053] - train_loss: 0.2276 -[wd: 4.00e-02] [lr: 1.88e-05][max mem allocated: 3.80e+04] (3114.5 ms)
INFO:root:[18,   640/ 1053] - train_loss: 0.2274 -[wd: 4.00e-02] [lr: 1.88e-05][max mem allocated: 3.80e+04] (3114.6 ms)
INFO:root:[18,   768/ 1053] - train_loss: 0.2273 -[wd: 4.00e-02] [lr: 1.89e-05][max mem allocated: 3.80e+04] (3115.0 ms)
INFO:root:[18,   896/ 1053] - train_loss: 0.2271 -[wd: 4.00e-02] [lr: 1.90e-05][max mem allocated: 3.80e+04] (3115.9 ms)
INFO:root:[18,  1024/ 1053] - train_loss: 0.2270 -[wd: 4.00e-02] [lr: 1.90e-05][max mem allocated: 3.80e+04] (3118.2 ms)
INFO:root:Loss 0.2257
INFO:root:Epoch 19
INFO:root:[19,     0/ 1053] - train_loss: 0.2257 -[wd: 4.00e-02] [lr: 1.90e-05][max mem allocated: 3.80e+04] (2945.3 ms)
