INFO:root:called-params config/configs.yaml
INFO:root:loaded params...
INFO:root:{'experiment_code': 3, 'k_means': {'K': 7806}, 'patches': {'N': 128}, 'num_workers': 16, 'base_dir': '/home/rtcalumby/adam/luciano/PlantCLEF2025/PlantCLEF2025/', 'pretrained_path': 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar', 'data': {'class_mapping': 'pretrained_models/class_mapping.txt', 'species_mapping': 'pretrained_models/species_id_to_name.txt', 'test_data': '/home/rtcalumby/adam/luciano/PlantCLEF2025/test_dataset/'}, 'batch_size': 1, 'gradient_accumulation': 128, 'optimization': {'epochs': 50, 'final_lr': 1e-06, 'final_weight_decay': 0.4, 'ipe_scale': 1.0, 'lr': 0.001, 'start_lr': 5e-06, 'warmup': 20, 'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/2)
INFO:root:Initialized (rank/world-size) 0/2
INFO:timm.models._helpers:Loaded state_dict_ema from checkpoint 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar'
Test dataset created for rank 0 world_size:  2
Test dataset, length: 1053
INFO:root:Loading Vision Transformer: PilotVisionTransformer(
  (patch_embed): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (patch_drop): Identity()
    (norm_pre): Identity()
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (fc_norm): Identity()
    (head_drop): Dropout(p=0.0, inplace=False)
    (head): Identity()
  )
  (linear): Linear(in_features=841, out_features=7806, bias=True)
  (blocks): ModuleList(
    (0-5): 6 x Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
INFO:root:Using AdamW
Allocated Memory with model loading: 1.0151658058166504  GB
INFO:root:Epoch 1
Rank 1
Test dataset created for rank 1 world_size:  2
Test dataset, length: 1053
Allocated Memory with model loading: 1.0151658058166504  GB
INFO:root:[1,     0/ 1053] - train_loss: 0.5630 -[wd: 1.00e-02] [lr: 1.00e-03][max mem allocated: 3.09e+04] (4851.2 ms)
INFO:root:[1,   128/ 1053] - train_loss: 0.5631 -[wd: 4.00e-02] [lr: 1.12e-05][max mem allocated: 3.13e+04] (2384.0 ms)
INFO:root:[1,   256/ 1053] - train_loss: 0.5602 -[wd: 4.00e-02] [lr: 1.74e-05][max mem allocated: 3.15e+04] (2384.2 ms)
INFO:root:[1,   384/ 1053] - train_loss: 0.5559 -[wd: 4.00e-02] [lr: 2.37e-05][max mem allocated: 3.15e+04] (2383.0 ms)
INFO:root:[1,   512/ 1053] - train_loss: 0.5503 -[wd: 4.01e-02] [lr: 2.99e-05][max mem allocated: 3.15e+04] (2383.3 ms)
INFO:root:[1,   640/ 1053] - train_loss: 0.5432 -[wd: 4.01e-02] [lr: 3.61e-05][max mem allocated: 3.15e+04] (2383.0 ms)
INFO:root:[1,   768/ 1053] - train_loss: 0.5347 -[wd: 4.02e-02] [lr: 4.23e-05][max mem allocated: 3.15e+04] (2382.7 ms)
INFO:root:[1,   896/ 1053] - train_loss: 0.5249 -[wd: 4.03e-02] [lr: 4.85e-05][max mem allocated: 3.15e+04] (2382.9 ms)
INFO:root:[1,  1024/ 1053] - train_loss: 0.5140 -[wd: 4.04e-02] [lr: 5.48e-05][max mem allocated: 3.15e+04] (2383.1 ms)
INFO:root:Loss 0.4104
INFO:root:Epoch 2
INFO:root:[2,     0/ 1053] - train_loss: 0.4074 -[wd: 4.04e-02] [lr: 5.48e-05][max mem allocated: 3.15e+04] (2530.4 ms)
INFO:root:[2,   128/ 1053] - train_loss: 0.4091 -[wd: 4.04e-02] [lr: 6.10e-05][max mem allocated: 3.15e+04] (2388.3 ms)
INFO:root:[2,   256/ 1053] - train_loss: 0.3952 -[wd: 4.06e-02] [lr: 6.72e-05][max mem allocated: 3.15e+04] (2386.1 ms)
INFO:root:[2,   384/ 1053] - train_loss: 0.3820 -[wd: 4.07e-02] [lr: 7.34e-05][max mem allocated: 3.15e+04] (2384.3 ms)
INFO:root:[2,   512/ 1053] - train_loss: 0.3695 -[wd: 4.08e-02] [lr: 7.96e-05][max mem allocated: 3.15e+04] (2385.4 ms)
INFO:root:[2,   640/ 1053] - train_loss: 0.3580 -[wd: 4.09e-02] [lr: 8.58e-05][max mem allocated: 3.15e+04] (2384.9 ms)
INFO:root:[2,   768/ 1053] - train_loss: 0.3476 -[wd: 4.11e-02] [lr: 9.21e-05][max mem allocated: 3.15e+04] (2384.5 ms)
INFO:root:[2,   896/ 1053] - train_loss: 0.3382 -[wd: 4.12e-02] [lr: 9.83e-05][max mem allocated: 3.15e+04] (2384.0 ms)
INFO:root:[2,  1024/ 1053] - train_loss: 0.3299 -[wd: 4.14e-02] [lr: 1.05e-04][max mem allocated: 3.15e+04] (2383.9 ms)
INFO:root:Loss 0.2652
INFO:root:Epoch 3
INFO:root:[3,     0/ 1053] - train_loss: 0.2650 -[wd: 4.14e-02] [lr: 1.05e-04][max mem allocated: 3.15e+04] (2502.4 ms)
INFO:root:[3,   128/ 1053] - train_loss: 0.2650 -[wd: 4.16e-02] [lr: 1.11e-04][max mem allocated: 3.15e+04] (2380.6 ms)
INFO:root:[3,   256/ 1053] - train_loss: 0.2625 -[wd: 4.18e-02] [lr: 1.17e-04][max mem allocated: 3.15e+04] (2385.4 ms)
INFO:root:[3,   384/ 1053] - train_loss: 0.2608 -[wd: 4.20e-02] [lr: 1.23e-04][max mem allocated: 3.15e+04] (2386.5 ms)
INFO:root:[3,   512/ 1053] - train_loss: 0.2596 -[wd: 4.22e-02] [lr: 1.29e-04][max mem allocated: 3.15e+04] (2384.0 ms)
INFO:root:[3,   640/ 1053] - train_loss: 0.2589 -[wd: 4.24e-02] [lr: 1.36e-04][max mem allocated: 3.15e+04] (2384.1 ms)
INFO:root:[3,   768/ 1053] - train_loss: 0.2585 -[wd: 4.27e-02] [lr: 1.42e-04][max mem allocated: 3.15e+04] (2383.5 ms)
INFO:root:[3,   896/ 1053] - train_loss: 0.2584 -[wd: 4.29e-02] [lr: 1.48e-04][max mem allocated: 3.15e+04] (2383.0 ms)
INFO:root:[3,  1024/ 1053] - train_loss: 0.2584 -[wd: 4.32e-02] [lr: 1.54e-04][max mem allocated: 3.15e+04] (2382.9 ms)
INFO:root:Loss 0.2584
INFO:root:Epoch 4
INFO:root:[4,     0/ 1053] - train_loss: 0.2584 -[wd: 4.32e-02] [lr: 1.54e-04][max mem allocated: 3.15e+04] (2652.6 ms)
INFO:root:[4,   128/ 1053] - train_loss: 0.2584 -[wd: 4.35e-02] [lr: 1.60e-04][max mem allocated: 3.15e+04] (2386.1 ms)
INFO:root:[4,   256/ 1053] - train_loss: 0.2581 -[wd: 4.37e-02] [lr: 1.67e-04][max mem allocated: 3.15e+04] (2386.3 ms)
INFO:root:[4,   384/ 1053] - train_loss: 0.2576 -[wd: 4.40e-02] [lr: 1.73e-04][max mem allocated: 3.15e+04] (2386.1 ms)
INFO:root:[4,   512/ 1053] - train_loss: 0.2571 -[wd: 4.43e-02] [lr: 1.79e-04][max mem allocated: 3.15e+04] (2386.1 ms)
INFO:root:[4,   640/ 1053] - train_loss: 0.2564 -[wd: 4.46e-02] [lr: 1.85e-04][max mem allocated: 3.15e+04] (2385.9 ms)
INFO:root:[4,   768/ 1053] - train_loss: 0.2557 -[wd: 4.50e-02] [lr: 1.92e-04][max mem allocated: 3.15e+04] (2384.5 ms)
INFO:root:[4,   896/ 1053] - train_loss: 0.2550 -[wd: 4.53e-02] [lr: 1.98e-04][max mem allocated: 3.15e+04] (2384.4 ms)
INFO:root:[4,  1024/ 1053] - train_loss: 0.2542 -[wd: 4.57e-02] [lr: 2.04e-04][max mem allocated: 3.15e+04] (2385.2 ms)
INFO:root:Loss 0.2466
INFO:root:Epoch 5
INFO:root:[5,     0/ 1053] - train_loss: 0.2466 -[wd: 4.57e-02] [lr: 2.04e-04][max mem allocated: 3.15e+04] (2619.8 ms)
INFO:root:[5,   128/ 1053] - train_loss: 0.2466 -[wd: 4.60e-02] [lr: 2.10e-04][max mem allocated: 3.15e+04] (2375.9 ms)
INFO:root:[5,   256/ 1053] - train_loss: 0.2455 -[wd: 4.64e-02] [lr: 2.16e-04][max mem allocated: 3.15e+04] (2377.0 ms)
INFO:root:[5,   384/ 1053] - train_loss: 0.2444 -[wd: 4.68e-02] [lr: 2.23e-04][max mem allocated: 3.15e+04] (2377.8 ms)
INFO:root:[5,   512/ 1053] - train_loss: 0.2435 -[wd: 4.71e-02] [lr: 2.29e-04][max mem allocated: 3.15e+04] (2377.4 ms)
INFO:root:[5,   640/ 1053] - train_loss: 0.2426 -[wd: 4.75e-02] [lr: 2.35e-04][max mem allocated: 3.15e+04] (2379.4 ms)
INFO:root:[5,   768/ 1053] - train_loss: 0.2417 -[wd: 4.80e-02] [lr: 2.41e-04][max mem allocated: 3.15e+04] (2379.4 ms)
INFO:root:[5,   896/ 1053] - train_loss: 0.2409 -[wd: 4.84e-02] [lr: 2.48e-04][max mem allocated: 3.15e+04] (2379.0 ms)
INFO:root:[5,  1024/ 1053] - train_loss: 0.2402 -[wd: 4.88e-02] [lr: 2.54e-04][max mem allocated: 3.15e+04] (2379.0 ms)
INFO:root:Loss 0.2333
INFO:root:Epoch 6
INFO:root:[6,     0/ 1053] - train_loss: 0.2333 -[wd: 4.88e-02] [lr: 2.54e-04][max mem allocated: 3.15e+04] (2655.9 ms)
INFO:root:[6,   128/ 1053] - train_loss: 0.2333 -[wd: 4.93e-02] [lr: 2.60e-04][max mem allocated: 3.15e+04] (2383.8 ms)
INFO:root:[6,   256/ 1053] - train_loss: 0.2324 -[wd: 4.97e-02] [lr: 2.66e-04][max mem allocated: 3.15e+04] (2382.9 ms)
INFO:root:[6,   384/ 1053] - train_loss: 0.2315 -[wd: 5.02e-02] [lr: 2.72e-04][max mem allocated: 3.15e+04] (2383.8 ms)
INFO:root:[6,   512/ 1053] - train_loss: 0.2306 -[wd: 5.06e-02] [lr: 2.79e-04][max mem allocated: 3.15e+04] (2381.2 ms)
INFO:root:[6,   640/ 1053] - train_loss: 0.2297 -[wd: 5.11e-02] [lr: 2.85e-04][max mem allocated: 3.15e+04] (2381.4 ms)
INFO:root:[6,   768/ 1053] - train_loss: 0.2288 -[wd: 5.16e-02] [lr: 2.91e-04][max mem allocated: 3.15e+04] (2381.5 ms)
INFO:root:[6,   896/ 1053] - train_loss: 0.2278 -[wd: 5.21e-02] [lr: 2.97e-04][max mem allocated: 3.15e+04] (2381.4 ms)
INFO:root:[6,  1024/ 1053] - train_loss: 0.2269 -[wd: 5.26e-02] [lr: 3.04e-04][max mem allocated: 3.15e+04] (2381.1 ms)
INFO:root:Loss 0.2210
INFO:root:Epoch 7
INFO:root:[7,     0/ 1053] - train_loss: 0.2208 -[wd: 5.26e-02] [lr: 3.04e-04][max mem allocated: 3.15e+04] (2531.8 ms)
INFO:root:[7,   128/ 1053] - train_loss: 0.2210 -[wd: 5.32e-02] [lr: 3.10e-04][max mem allocated: 3.15e+04] (2376.5 ms)
INFO:root:[7,   256/ 1053] - train_loss: 0.2201 -[wd: 5.37e-02] [lr: 3.16e-04][max mem allocated: 3.15e+04] (2375.5 ms)
INFO:root:[7,   384/ 1053] - train_loss: 0.2195 -[wd: 5.42e-02] [lr: 3.22e-04][max mem allocated: 3.15e+04] (2376.2 ms)
INFO:root:[7,   512/ 1053] - train_loss: 0.2191 -[wd: 5.48e-02] [lr: 3.28e-04][max mem allocated: 3.15e+04] (2377.3 ms)
INFO:root:[7,   640/ 1053] - train_loss: 0.2188 -[wd: 5.54e-02] [lr: 3.35e-04][max mem allocated: 3.15e+04] (2377.8 ms)
INFO:root:[7,   768/ 1053] - train_loss: 0.2186 -[wd: 5.59e-02] [lr: 3.41e-04][max mem allocated: 3.15e+04] (2378.8 ms)
INFO:root:[7,   896/ 1053] - train_loss: 0.2184 -[wd: 5.65e-02] [lr: 3.47e-04][max mem allocated: 3.15e+04] (2378.8 ms)
INFO:root:[7,  1024/ 1053] - train_loss: 0.2182 -[wd: 5.71e-02] [lr: 3.53e-04][max mem allocated: 3.15e+04] (2379.6 ms)
INFO:root:Loss 0.2167
INFO:root:Epoch 8
INFO:root:[8,     0/ 1053] - train_loss: 0.2167 -[wd: 5.71e-02] [lr: 3.53e-04][max mem allocated: 3.15e+04] (2570.4 ms)
INFO:root:[8,   128/ 1053] - train_loss: 0.2167 -[wd: 5.77e-02] [lr: 3.59e-04][max mem allocated: 3.15e+04] (2378.2 ms)
INFO:root:[8,   256/ 1053] - train_loss: 0.2166 -[wd: 5.84e-02] [lr: 3.66e-04][max mem allocated: 3.15e+04] (2381.4 ms)
INFO:root:[8,   384/ 1053] - train_loss: 0.2164 -[wd: 5.90e-02] [lr: 3.72e-04][max mem allocated: 3.15e+04] (2380.0 ms)
INFO:root:[8,   512/ 1053] - train_loss: 0.2163 -[wd: 5.96e-02] [lr: 3.78e-04][max mem allocated: 3.15e+04] (2379.0 ms)
INFO:root:[8,   640/ 1053] - train_loss: 0.2163 -[wd: 6.03e-02] [lr: 3.84e-04][max mem allocated: 3.15e+04] (2380.5 ms)
INFO:root:[8,   768/ 1053] - train_loss: 0.2162 -[wd: 6.09e-02] [lr: 3.91e-04][max mem allocated: 3.15e+04] (2381.1 ms)
INFO:root:[8,   896/ 1053] - train_loss: 0.2161 -[wd: 6.16e-02] [lr: 3.97e-04][max mem allocated: 3.15e+04] (2382.1 ms)
INFO:root:[8,  1024/ 1053] - train_loss: 0.2160 -[wd: 6.23e-02] [lr: 4.03e-04][max mem allocated: 3.15e+04] (2382.3 ms)
INFO:root:Loss 0.2155
INFO:root:Epoch 9
INFO:root:[9,     0/ 1053] - train_loss: 0.2155 -[wd: 6.23e-02] [lr: 4.03e-04][max mem allocated: 3.15e+04] (2495.7 ms)
INFO:root:[9,   128/ 1053] - train_loss: 0.2155 -[wd: 6.30e-02] [lr: 4.09e-04][max mem allocated: 3.15e+04] (2378.0 ms)
INFO:root:[9,   256/ 1053] - train_loss: 0.2154 -[wd: 6.36e-02] [lr: 4.15e-04][max mem allocated: 3.15e+04] (2378.1 ms)
INFO:root:[9,   384/ 1053] - train_loss: 0.2153 -[wd: 6.44e-02] [lr: 4.22e-04][max mem allocated: 3.15e+04] (2378.7 ms)
INFO:root:[9,   512/ 1053] - train_loss: 0.2153 -[wd: 6.51e-02] [lr: 4.28e-04][max mem allocated: 3.15e+04] (2379.6 ms)
INFO:root:[9,   640/ 1053] - train_loss: 0.2152 -[wd: 6.58e-02] [lr: 4.34e-04][max mem allocated: 3.15e+04] (2380.1 ms)
INFO:root:[9,   768/ 1053] - train_loss: 0.2152 -[wd: 6.65e-02] [lr: 4.40e-04][max mem allocated: 3.15e+04] (2381.0 ms)
INFO:root:[9,   896/ 1053] - train_loss: 0.2152 -[wd: 6.73e-02] [lr: 4.47e-04][max mem allocated: 3.15e+04] (2381.5 ms)
INFO:root:[9,  1024/ 1053] - train_loss: 0.2151 -[wd: 6.80e-02] [lr: 4.53e-04][max mem allocated: 3.15e+04] (2382.5 ms)
INFO:root:Loss 0.2148
INFO:root:Epoch 10
INFO:root:[10,     0/ 1053] - train_loss: 0.2148 -[wd: 6.80e-02] [lr: 4.53e-04][max mem allocated: 3.15e+04] (2486.1 ms)
INFO:root:[10,   128/ 1053] - train_loss: 0.2148 -[wd: 6.88e-02] [lr: 4.59e-04][max mem allocated: 3.15e+04] (2382.1 ms)
INFO:root:[10,   256/ 1053] - train_loss: 0.2148 -[wd: 6.96e-02] [lr: 4.65e-04][max mem allocated: 3.15e+04] (2380.6 ms)
INFO:root:[10,   384/ 1053] - train_loss: 0.2148 -[wd: 7.03e-02] [lr: 4.71e-04][max mem allocated: 3.15e+04] (2380.2 ms)
INFO:root:[10,   512/ 1053] - train_loss: 0.2148 -[wd: 7.11e-02] [lr: 4.78e-04][max mem allocated: 3.15e+04] (2380.4 ms)
INFO:root:[10,   640/ 1053] - train_loss: 0.2147 -[wd: 7.19e-02] [lr: 4.84e-04][max mem allocated: 3.15e+04] (2382.7 ms)
INFO:root:[10,   768/ 1053] - train_loss: 0.2147 -[wd: 7.27e-02] [lr: 4.90e-04][max mem allocated: 3.15e+04] (2383.9 ms)
INFO:root:[10,   896/ 1053] - train_loss: 0.2147 -[wd: 7.36e-02] [lr: 4.96e-04][max mem allocated: 3.15e+04] (2383.5 ms)
INFO:root:[10,  1024/ 1053] - train_loss: 0.2147 -[wd: 7.44e-02] [lr: 5.03e-04][max mem allocated: 3.15e+04] (2383.0 ms)
INFO:root:Loss 0.2146
INFO:root:Epoch 11
INFO:root:[11,     0/ 1053] - train_loss: 0.2146 -[wd: 7.44e-02] [lr: 5.03e-04][max mem allocated: 3.15e+04] (2726.3 ms)
INFO:root:[11,   128/ 1053] - train_loss: 0.2146 -[wd: 7.52e-02] [lr: 5.09e-04][max mem allocated: 3.15e+04] (2382.2 ms)
INFO:root:[11,   256/ 1053] - train_loss: 0.2145 -[wd: 7.61e-02] [lr: 5.15e-04][max mem allocated: 3.15e+04] (2383.9 ms)
INFO:root:[11,   384/ 1053] - train_loss: 0.2145 -[wd: 7.69e-02] [lr: 5.21e-04][max mem allocated: 3.15e+04] (2383.4 ms)
INFO:root:[11,   512/ 1053] - train_loss: 0.2145 -[wd: 7.78e-02] [lr: 5.27e-04][max mem allocated: 3.15e+04] (2383.1 ms)
INFO:root:[11,   640/ 1053] - train_loss: 0.2145 -[wd: 7.86e-02] [lr: 5.34e-04][max mem allocated: 3.15e+04] (2383.1 ms)
INFO:root:[11,   768/ 1053] - train_loss: 0.2145 -[wd: 7.95e-02] [lr: 5.40e-04][max mem allocated: 3.15e+04] (2383.3 ms)
INFO:root:[11,   896/ 1053] - train_loss: 0.2145 -[wd: 8.04e-02] [lr: 5.46e-04][max mem allocated: 3.15e+04] (2383.3 ms)
INFO:root:[11,  1024/ 1053] - train_loss: 0.2145 -[wd: 8.13e-02] [lr: 5.52e-04][max mem allocated: 3.15e+04] (2383.5 ms)
INFO:root:Loss 0.2144
INFO:root:Epoch 12
INFO:root:[12,     0/ 1053] - train_loss: 0.2144 -[wd: 8.13e-02] [lr: 5.52e-04][max mem allocated: 3.15e+04] (2591.6 ms)
INFO:root:[12,   128/ 1053] - train_loss: 0.2144 -[wd: 8.22e-02] [lr: 5.58e-04][max mem allocated: 3.15e+04] (2376.8 ms)
INFO:root:[12,   256/ 1053] - train_loss: 0.2144 -[wd: 8.31e-02] [lr: 5.65e-04][max mem allocated: 3.15e+04] (2378.2 ms)
INFO:root:[12,   384/ 1053] - train_loss: 0.2144 -[wd: 8.40e-02] [lr: 5.71e-04][max mem allocated: 3.15e+04] (2380.9 ms)
INFO:root:[12,   512/ 1053] - train_loss: 0.2144 -[wd: 8.50e-02] [lr: 5.77e-04][max mem allocated: 3.15e+04] (2382.2 ms)
INFO:root:[12,   640/ 1053] - train_loss: 0.2144 -[wd: 8.59e-02] [lr: 5.83e-04][max mem allocated: 3.15e+04] (2382.9 ms)
INFO:root:[12,   768/ 1053] - train_loss: 0.2144 -[wd: 8.69e-02] [lr: 5.90e-04][max mem allocated: 3.15e+04] (2382.4 ms)
INFO:root:[12,   896/ 1053] - train_loss: 0.2144 -[wd: 8.78e-02] [lr: 5.96e-04][max mem allocated: 3.15e+04] (2383.1 ms)
INFO:root:[12,  1024/ 1053] - train_loss: 0.2144 -[wd: 8.88e-02] [lr: 6.02e-04][max mem allocated: 3.15e+04] (2383.5 ms)
INFO:root:Loss 0.2143
INFO:root:Epoch 13
INFO:root:[13,     0/ 1053] - train_loss: 0.2143 -[wd: 8.88e-02] [lr: 6.02e-04][max mem allocated: 3.15e+04] (2508.3 ms)
INFO:root:[13,   128/ 1053] - train_loss: 0.2143 -[wd: 8.98e-02] [lr: 6.08e-04][max mem allocated: 3.15e+04] (2378.5 ms)
INFO:root:[13,   256/ 1053] - train_loss: 0.2144 -[wd: 9.07e-02] [lr: 6.14e-04][max mem allocated: 3.15e+04] (2383.3 ms)
INFO:root:[13,   384/ 1053] - train_loss: 0.2144 -[wd: 9.17e-02] [lr: 6.21e-04][max mem allocated: 3.15e+04] (2384.7 ms)
INFO:root:[13,   512/ 1053] - train_loss: 0.2144 -[wd: 9.27e-02] [lr: 6.27e-04][max mem allocated: 3.15e+04] (2383.3 ms)
INFO:root:[13,   640/ 1053] - train_loss: 0.2144 -[wd: 9.37e-02] [lr: 6.33e-04][max mem allocated: 3.15e+04] (2382.4 ms)
INFO:root:[13,   768/ 1053] - train_loss: 0.2144 -[wd: 9.47e-02] [lr: 6.39e-04][max mem allocated: 3.15e+04] (2382.3 ms)
INFO:root:[13,   896/ 1053] - train_loss: 0.2144 -[wd: 9.58e-02] [lr: 6.46e-04][max mem allocated: 3.15e+04] (2382.3 ms)
INFO:root:[13,  1024/ 1053] - train_loss: 0.2143 -[wd: 9.68e-02] [lr: 6.52e-04][max mem allocated: 3.15e+04] (2382.4 ms)
INFO:root:Loss 0.2143
INFO:root:Epoch 14
INFO:root:[14,     0/ 1053] - train_loss: 0.2143 -[wd: 9.68e-02] [lr: 6.52e-04][max mem allocated: 3.15e+04] (2521.3 ms)
INFO:root:[14,   128/ 1053] - train_loss: 0.2143 -[wd: 9.78e-02] [lr: 6.58e-04][max mem allocated: 3.15e+04] (2378.0 ms)
INFO:root:[14,   256/ 1053] - train_loss: 0.2144 -[wd: 9.89e-02] [lr: 6.64e-04][max mem allocated: 3.15e+04] (2380.0 ms)
INFO:root:[14,   384/ 1053] - train_loss: 0.2144 -[wd: 9.99e-02] [lr: 6.70e-04][max mem allocated: 3.15e+04] (2383.7 ms)
INFO:root:[14,   512/ 1053] - train_loss: 0.2143 -[wd: 1.01e-01] [lr: 6.77e-04][max mem allocated: 3.15e+04] (2384.2 ms)
INFO:root:[14,   640/ 1053] - train_loss: 0.2143 -[wd: 1.02e-01] [lr: 6.83e-04][max mem allocated: 3.15e+04] (2385.0 ms)
INFO:root:[14,   768/ 1053] - train_loss: 0.2143 -[wd: 1.03e-01] [lr: 6.89e-04][max mem allocated: 3.15e+04] (2385.7 ms)
INFO:root:[14,   896/ 1053] - train_loss: 0.2143 -[wd: 1.04e-01] [lr: 6.95e-04][max mem allocated: 3.15e+04] (2386.2 ms)
INFO:root:[14,  1024/ 1053] - train_loss: 0.2143 -[wd: 1.05e-01] [lr: 7.01e-04][max mem allocated: 3.15e+04] (2385.9 ms)
INFO:root:Loss 0.2143
INFO:root:Epoch 15
INFO:root:[15,     0/ 1053] - train_loss: 0.2143 -[wd: 1.05e-01] [lr: 7.01e-04][max mem allocated: 3.15e+04] (2692.6 ms)
INFO:root:[15,   128/ 1053] - train_loss: 0.2143 -[wd: 1.06e-01] [lr: 7.08e-04][max mem allocated: 3.15e+04] (2379.1 ms)
INFO:root:[15,   256/ 1053] - train_loss: 0.2143 -[wd: 1.07e-01] [lr: 7.14e-04][max mem allocated: 3.15e+04] (2381.0 ms)
INFO:root:[15,   384/ 1053] - train_loss: 0.2143 -[wd: 1.09e-01] [lr: 7.20e-04][max mem allocated: 3.15e+04] (2380.4 ms)
INFO:root:[15,   512/ 1053] - train_loss: 0.2143 -[wd: 1.10e-01] [lr: 7.26e-04][max mem allocated: 3.15e+04] (2382.7 ms)
INFO:root:[15,   640/ 1053] - train_loss: 0.2143 -[wd: 1.11e-01] [lr: 7.33e-04][max mem allocated: 3.15e+04] (2382.8 ms)
INFO:root:[15,   768/ 1053] - train_loss: 0.2143 -[wd: 1.12e-01] [lr: 7.39e-04][max mem allocated: 3.15e+04] (2383.7 ms)
INFO:root:[15,   896/ 1053] - train_loss: 0.2143 -[wd: 1.13e-01] [lr: 7.45e-04][max mem allocated: 3.15e+04] (2385.0 ms)
INFO:root:[15,  1024/ 1053] - train_loss: 0.2143 -[wd: 1.14e-01] [lr: 7.51e-04][max mem allocated: 3.15e+04] (2385.7 ms)
INFO:root:Loss 0.2142
INFO:root:Epoch 16
INFO:root:[16,     0/ 1053] - train_loss: 0.2142 -[wd: 1.14e-01] [lr: 7.51e-04][max mem allocated: 3.15e+04] (2721.9 ms)
INFO:root:[16,   128/ 1053] - train_loss: 0.2142 -[wd: 1.15e-01] [lr: 7.57e-04][max mem allocated: 3.15e+04] (2384.2 ms)
INFO:root:[16,   256/ 1053] - train_loss: 0.2143 -[wd: 1.16e-01] [lr: 7.64e-04][max mem allocated: 3.15e+04] (2387.3 ms)
INFO:root:[16,   384/ 1053] - train_loss: 0.2143 -[wd: 1.18e-01] [lr: 7.70e-04][max mem allocated: 3.15e+04] (2389.7 ms)
INFO:root:[16,   512/ 1053] - train_loss: 0.2143 -[wd: 1.19e-01] [lr: 7.76e-04][max mem allocated: 3.15e+04] (2389.6 ms)
INFO:root:[16,   640/ 1053] - train_loss: 0.2143 -[wd: 1.20e-01] [lr: 7.82e-04][max mem allocated: 3.15e+04] (2388.7 ms)
INFO:root:[16,   768/ 1053] - train_loss: 0.2143 -[wd: 1.21e-01] [lr: 7.89e-04][max mem allocated: 3.15e+04] (2388.8 ms)
INFO:root:[16,   896/ 1053] - train_loss: 0.2143 -[wd: 1.22e-01] [lr: 7.95e-04][max mem allocated: 3.15e+04] (2389.3 ms)
INFO:root:[16,  1024/ 1053] - train_loss: 0.2143 -[wd: 1.24e-01] [lr: 8.01e-04][max mem allocated: 3.15e+04] (2389.4 ms)
INFO:root:Loss 0.2142
INFO:root:Epoch 17
INFO:root:[17,     0/ 1053] - train_loss: 0.2142 -[wd: 1.24e-01] [lr: 8.01e-04][max mem allocated: 3.15e+04] (2631.9 ms)
INFO:root:[17,   128/ 1053] - train_loss: 0.2142 -[wd: 1.25e-01] [lr: 8.07e-04][max mem allocated: 3.15e+04] (2396.1 ms)
INFO:root:[17,   256/ 1053] - train_loss: 0.2142 -[wd: 1.26e-01] [lr: 8.13e-04][max mem allocated: 3.15e+04] (2395.5 ms)
INFO:root:[17,   384/ 1053] - train_loss: 0.2142 -[wd: 1.27e-01] [lr: 8.20e-04][max mem allocated: 3.15e+04] (2395.7 ms)
INFO:root:[17,   512/ 1053] - train_loss: 0.2142 -[wd: 1.28e-01] [lr: 8.26e-04][max mem allocated: 3.15e+04] (2396.3 ms)
INFO:root:[17,   640/ 1053] - train_loss: 0.2142 -[wd: 1.30e-01] [lr: 8.32e-04][max mem allocated: 3.15e+04] (2397.0 ms)
INFO:root:[17,   768/ 1053] - train_loss: 0.2142 -[wd: 1.31e-01] [lr: 8.38e-04][max mem allocated: 3.15e+04] (2397.9 ms)
INFO:root:[17,   896/ 1053] - train_loss: 0.2142 -[wd: 1.32e-01] [lr: 8.45e-04][max mem allocated: 3.15e+04] (2397.7 ms)
INFO:root:[17,  1024/ 1053] - train_loss: 0.2141 -[wd: 1.33e-01] [lr: 8.51e-04][max mem allocated: 3.15e+04] (2397.5 ms)
INFO:root:Loss 0.2140
INFO:root:Epoch 18
INFO:root:[18,     0/ 1053] - train_loss: 0.2140 -[wd: 1.33e-01] [lr: 8.51e-04][max mem allocated: 3.15e+04] (2564.6 ms)
INFO:root:[18,   128/ 1053] - train_loss: 0.2140 -[wd: 1.35e-01] [lr: 8.57e-04][max mem allocated: 3.15e+04] (2389.2 ms)
INFO:root:[18,   256/ 1053] - train_loss: 0.2141 -[wd: 1.36e-01] [lr: 8.63e-04][max mem allocated: 3.15e+04] (2393.7 ms)
INFO:root:[18,   384/ 1053] - train_loss: 0.2140 -[wd: 1.37e-01] [lr: 8.69e-04][max mem allocated: 3.15e+04] (2397.0 ms)
INFO:root:[18,   512/ 1053] - train_loss: 0.2140 -[wd: 1.38e-01] [lr: 8.76e-04][max mem allocated: 3.15e+04] (2399.3 ms)
INFO:root:[18,   640/ 1053] - train_loss: 0.2140 -[wd: 1.40e-01] [lr: 8.82e-04][max mem allocated: 3.15e+04] (2399.1 ms)
INFO:root:[18,   768/ 1053] - train_loss: 0.2140 -[wd: 1.41e-01] [lr: 8.88e-04][max mem allocated: 3.15e+04] (2399.4 ms)
INFO:root:[18,   896/ 1053] - train_loss: 0.2139 -[wd: 1.42e-01] [lr: 8.94e-04][max mem allocated: 3.15e+04] (2400.1 ms)
INFO:root:[18,  1024/ 1053] - train_loss: 0.2139 -[wd: 1.43e-01] [lr: 9.01e-04][max mem allocated: 3.15e+04] (2399.5 ms)
INFO:root:Loss 0.2136
INFO:root:Epoch 19
INFO:root:[19,     0/ 1053] - train_loss: 0.2136 -[wd: 1.43e-01] [lr: 9.01e-04][max mem allocated: 3.15e+04] (2475.4 ms)
INFO:root:[19,   128/ 1053] - train_loss: 0.2136 -[wd: 1.45e-01] [lr: 9.07e-04][max mem allocated: 3.15e+04] (2372.4 ms)
INFO:root:[19,   256/ 1053] - train_loss: 0.2137 -[wd: 1.46e-01] [lr: 9.13e-04][max mem allocated: 3.15e+04] (2377.5 ms)
INFO:root:[19,   384/ 1053] - train_loss: 0.2136 -[wd: 1.47e-01] [lr: 9.19e-04][max mem allocated: 3.15e+04] (2381.7 ms)
INFO:root:[19,   512/ 1053] - train_loss: 0.2136 -[wd: 1.49e-01] [lr: 9.25e-04][max mem allocated: 3.15e+04] (2383.4 ms)
INFO:root:[19,   640/ 1053] - train_loss: 0.2135 -[wd: 1.50e-01] [lr: 9.32e-04][max mem allocated: 3.15e+04] (2383.7 ms)
INFO:root:[19,   768/ 1053] - train_loss: 0.2135 -[wd: 1.51e-01] [lr: 9.38e-04][max mem allocated: 3.15e+04] (2384.8 ms)
INFO:root:[19,   896/ 1053] - train_loss: 0.2134 -[wd: 1.52e-01] [lr: 9.44e-04][max mem allocated: 3.15e+04] (2386.0 ms)
INFO:root:[19,  1024/ 1053] - train_loss: 0.2134 -[wd: 1.54e-01] [lr: 9.50e-04][max mem allocated: 3.15e+04] (2386.8 ms)
INFO:root:Loss 0.2128
INFO:root:Epoch 20
INFO:root:[20,     0/ 1053] - train_loss: 0.2127 -[wd: 1.54e-01] [lr: 9.50e-04][max mem allocated: 3.15e+04] (2551.0 ms)
INFO:root:[20,   128/ 1053] - train_loss: 0.2128 -[wd: 1.55e-01] [lr: 9.56e-04][max mem allocated: 3.15e+04] (2392.1 ms)
INFO:root:[20,   256/ 1053] - train_loss: 0.2167 -[wd: 1.56e-01] [lr: 9.63e-04][max mem allocated: 3.15e+04] (2390.4 ms)
INFO:root:[20,   384/ 1053] - train_loss: 0.2160 -[wd: 1.58e-01] [lr: 9.69e-04][max mem allocated: 3.15e+04] (2389.6 ms)
INFO:root:[20,   512/ 1053] - train_loss: 0.2158 -[wd: 1.59e-01] [lr: 9.75e-04][max mem allocated: 3.15e+04] (2389.0 ms)
INFO:root:[20,   640/ 1053] - train_loss: 0.2158 -[wd: 1.60e-01] [lr: 9.81e-04][max mem allocated: 3.15e+04] (2389.7 ms)
INFO:root:[20,   768/ 1053] - train_loss: 0.2158 -[wd: 1.62e-01] [lr: 9.88e-04][max mem allocated: 3.15e+04] (2389.7 ms)
INFO:root:[20,   896/ 1053] - train_loss: 0.2157 -[wd: 1.63e-01] [lr: 9.94e-04][max mem allocated: 3.15e+04] (2390.5 ms)
INFO:root:[20,  1024/ 1053] - train_loss: 0.2156 -[wd: 1.64e-01] [lr: 1.00e-03][max mem allocated: 3.15e+04] (2390.6 ms)
INFO:root:Loss 0.2148
INFO:root:Epoch 21
INFO:root:[21,     0/ 1053] - train_loss: 0.2148 -[wd: 1.64e-01] [lr: 1.00e-03][max mem allocated: 3.15e+04] (2526.6 ms)
INFO:root:[21,   128/ 1053] - train_loss: 0.2148 -[wd: 1.66e-01] [lr: 1.00e-03][max mem allocated: 3.15e+04] (2400.2 ms)
INFO:root:[21,   256/ 1053] - train_loss: 0.2149 -[wd: 1.67e-01] [lr: 1.00e-03][max mem allocated: 3.15e+04] (2395.6 ms)
INFO:root:[21,   384/ 1053] - train_loss: 0.2148 -[wd: 1.68e-01] [lr: 1.00e-03][max mem allocated: 3.15e+04] (2396.4 ms)
INFO:root:[21,   512/ 1053] - train_loss: 0.2148 -[wd: 1.70e-01] [lr: 9.99e-04][max mem allocated: 3.15e+04] (2399.1 ms)
INFO:root:[21,   640/ 1053] - train_loss: 0.2148 -[wd: 1.71e-01] [lr: 9.99e-04][max mem allocated: 3.15e+04] (2400.7 ms)
INFO:root:[21,   768/ 1053] - train_loss: 0.2148 -[wd: 1.73e-01] [lr: 9.98e-04][max mem allocated: 3.15e+04] (2400.6 ms)
INFO:root:[21,   896/ 1053] - train_loss: 0.2148 -[wd: 1.74e-01] [lr: 9.98e-04][max mem allocated: 3.15e+04] (2401.3 ms)
INFO:root:[21,  1024/ 1053] - train_loss: 0.2148 -[wd: 1.75e-01] [lr: 9.97e-04][max mem allocated: 3.15e+04] (2401.3 ms)
INFO:root:Loss 0.2145
INFO:root:Epoch 22
INFO:root:[22,     0/ 1053] - train_loss: 0.2145 -[wd: 1.75e-01] [lr: 9.97e-04][max mem allocated: 3.15e+04] (2615.5 ms)
INFO:root:[22,   128/ 1053] - train_loss: 0.2145 -[wd: 1.77e-01] [lr: 9.97e-04][max mem allocated: 3.15e+04] (2401.2 ms)
INFO:root:[22,   256/ 1053] - train_loss: 0.2146 -[wd: 1.78e-01] [lr: 9.96e-04][max mem allocated: 3.15e+04] (2400.6 ms)
INFO:root:[22,   384/ 1053] - train_loss: 0.2145 -[wd: 1.79e-01] [lr: 9.95e-04][max mem allocated: 3.15e+04] (2402.1 ms)
INFO:root:[22,   512/ 1053] - train_loss: 0.2145 -[wd: 1.81e-01] [lr: 9.94e-04][max mem allocated: 3.15e+04] (2404.1 ms)
INFO:root:[22,   640/ 1053] - train_loss: 0.2145 -[wd: 1.82e-01] [lr: 9.93e-04][max mem allocated: 3.15e+04] (2404.5 ms)
INFO:root:[22,   768/ 1053] - train_loss: 0.2145 -[wd: 1.83e-01] [lr: 9.92e-04][max mem allocated: 3.15e+04] (2404.8 ms)
INFO:root:[22,   896/ 1053] - train_loss: 0.2144 -[wd: 1.85e-01] [lr: 9.90e-04][max mem allocated: 3.15e+04] (2404.8 ms)
INFO:root:[22,  1024/ 1053] - train_loss: 0.2144 -[wd: 1.86e-01] [lr: 9.89e-04][max mem allocated: 3.15e+04] (2406.0 ms)
INFO:root:Loss 0.2143
INFO:root:Epoch 23
INFO:root:[23,     0/ 1053] - train_loss: 0.2143 -[wd: 1.86e-01] [lr: 9.89e-04][max mem allocated: 3.15e+04] (2565.3 ms)
INFO:root:[23,   128/ 1053] - train_loss: 0.2143 -[wd: 1.88e-01] [lr: 9.88e-04][max mem allocated: 3.15e+04] (2397.7 ms)
INFO:root:[23,   256/ 1053] - train_loss: 0.2143 -[wd: 1.89e-01] [lr: 9.86e-04][max mem allocated: 3.15e+04] (2406.2 ms)
INFO:root:[23,   384/ 1053] - train_loss: 0.2143 -[wd: 1.90e-01] [lr: 9.85e-04][max mem allocated: 3.15e+04] (2408.5 ms)
INFO:root:[23,   512/ 1053] - train_loss: 0.2143 -[wd: 1.92e-01] [lr: 9.83e-04][max mem allocated: 3.15e+04] (2408.6 ms)
INFO:root:[23,   640/ 1053] - train_loss: 0.2142 -[wd: 1.93e-01] [lr: 9.81e-04][max mem allocated: 3.15e+04] (2410.4 ms)
INFO:root:[23,   768/ 1053] - train_loss: 0.2142 -[wd: 1.95e-01] [lr: 9.79e-04][max mem allocated: 3.15e+04] (2410.8 ms)
INFO:root:[23,   896/ 1053] - train_loss: 0.2142 -[wd: 1.96e-01] [lr: 9.78e-04][max mem allocated: 3.15e+04] (2410.4 ms)
INFO:root:[23,  1024/ 1053] - train_loss: 0.2142 -[wd: 1.97e-01] [lr: 9.76e-04][max mem allocated: 3.15e+04] (2410.6 ms)
INFO:root:Loss 0.2140
INFO:root:Epoch 24
INFO:root:[24,     0/ 1053] - train_loss: 0.2140 -[wd: 1.97e-01] [lr: 9.76e-04][max mem allocated: 3.15e+04] (2937.8 ms)
INFO:root:[24,   128/ 1053] - train_loss: 0.2140 -[wd: 1.99e-01] [lr: 9.73e-04][max mem allocated: 3.15e+04] (2406.6 ms)
INFO:root:[24,   256/ 1053] - train_loss: 0.2141 -[wd: 2.00e-01] [lr: 9.71e-04][max mem allocated: 3.15e+04] (2407.6 ms)
INFO:root:[24,   384/ 1053] - train_loss: 0.2140 -[wd: 2.02e-01] [lr: 9.69e-04][max mem allocated: 3.15e+04] (2406.0 ms)
INFO:root:[24,   512/ 1053] - train_loss: 0.2140 -[wd: 2.03e-01] [lr: 9.67e-04][max mem allocated: 3.15e+04] (2405.6 ms)
INFO:root:[24,   640/ 1053] - train_loss: 0.2140 -[wd: 2.04e-01] [lr: 9.64e-04][max mem allocated: 3.15e+04] (2408.2 ms)
INFO:root:[24,   768/ 1053] - train_loss: 0.2140 -[wd: 2.06e-01] [lr: 9.62e-04][max mem allocated: 3.15e+04] (2410.7 ms)
INFO:root:[24,   896/ 1053] - train_loss: 0.2139 -[wd: 2.07e-01] [lr: 9.59e-04][max mem allocated: 3.15e+04] (2412.1 ms)
INFO:root:[24,  1024/ 1053] - train_loss: 0.2139 -[wd: 2.09e-01] [lr: 9.57e-04][max mem allocated: 3.15e+04] (2412.8 ms)
INFO:root:Loss 0.2137
INFO:root:Epoch 25
INFO:root:[25,     0/ 1053] - train_loss: 0.2136 -[wd: 2.09e-01] [lr: 9.57e-04][max mem allocated: 3.15e+04] (2565.8 ms)
INFO:root:[25,   128/ 1053] - train_loss: 0.2137 -[wd: 2.10e-01] [lr: 9.54e-04][max mem allocated: 3.15e+04] (2410.4 ms)
INFO:root:[25,   256/ 1053] - train_loss: 0.2136 -[wd: 2.12e-01] [lr: 9.51e-04][max mem allocated: 3.15e+04] (2410.9 ms)
INFO:root:[25,   384/ 1053] - train_loss: 0.2136 -[wd: 2.13e-01] [lr: 9.48e-04][max mem allocated: 3.15e+04] (2414.9 ms)
INFO:root:[25,   512/ 1053] - train_loss: 0.2135 -[wd: 2.14e-01] [lr: 9.46e-04][max mem allocated: 3.15e+04] (2416.8 ms)
INFO:root:[25,   640/ 1053] - train_loss: 0.2135 -[wd: 2.16e-01] [lr: 9.43e-04][max mem allocated: 3.15e+04] (2416.7 ms)
INFO:root:[25,   768/ 1053] - train_loss: 0.2134 -[wd: 2.17e-01] [lr: 9.39e-04][max mem allocated: 3.15e+04] (2416.9 ms)
INFO:root:[25,   896/ 1053] - train_loss: 0.2133 -[wd: 2.19e-01] [lr: 9.36e-04][max mem allocated: 3.15e+04] (2416.4 ms)
INFO:root:[25,  1024/ 1053] - train_loss: 0.2132 -[wd: 2.20e-01] [lr: 9.33e-04][max mem allocated: 3.15e+04] (2413.3 ms)
INFO:root:Loss 0.2124
INFO:root:Epoch 26
INFO:root:[26,     0/ 1053] - train_loss: 0.2124 -[wd: 2.20e-01] [lr: 9.33e-04][max mem allocated: 3.15e+04] (2564.6 ms)
INFO:root:[26,   128/ 1053] - train_loss: 0.2124 -[wd: 2.21e-01] [lr: 9.30e-04][max mem allocated: 3.15e+04] (2389.2 ms)
INFO:root:[26,   256/ 1053] - train_loss: 0.2123 -[wd: 2.23e-01] [lr: 9.26e-04][max mem allocated: 3.15e+04] (2387.6 ms)
INFO:root:[26,   384/ 1053] - train_loss: 0.2122 -[wd: 2.24e-01] [lr: 9.23e-04][max mem allocated: 3.15e+04] (2388.6 ms)
INFO:root:[26,   512/ 1053] - train_loss: 0.2121 -[wd: 2.26e-01] [lr: 9.19e-04][max mem allocated: 3.15e+04] (2388.8 ms)
INFO:root:[26,   640/ 1053] - train_loss: 0.2120 -[wd: 2.27e-01] [lr: 9.16e-04][max mem allocated: 3.15e+04] (2388.7 ms)
INFO:root:[26,   768/ 1053] - train_loss: 0.2119 -[wd: 2.28e-01] [lr: 9.12e-04][max mem allocated: 3.15e+04] (2388.7 ms)
INFO:root:[26,   896/ 1053] - train_loss: 0.2118 -[wd: 2.30e-01] [lr: 9.08e-04][max mem allocated: 3.15e+04] (2389.5 ms)
INFO:root:[26,  1024/ 1053] - train_loss: 0.2117 -[wd: 2.31e-01] [lr: 9.05e-04][max mem allocated: 3.15e+04] (2390.9 ms)
INFO:root:Loss 0.2109
INFO:root:Epoch 27
INFO:root:[27,     0/ 1053] - train_loss: 0.2108 -[wd: 2.31e-01] [lr: 9.05e-04][max mem allocated: 3.15e+04] (2627.8 ms)
INFO:root:[27,   128/ 1053] - train_loss: 0.2109 -[wd: 2.33e-01] [lr: 9.01e-04][max mem allocated: 3.15e+04] (2387.3 ms)
INFO:root:[27,   256/ 1053] - train_loss: 0.2108 -[wd: 2.34e-01] [lr: 8.97e-04][max mem allocated: 3.15e+04] (2386.1 ms)
INFO:root:[27,   384/ 1053] - train_loss: 0.2107 -[wd: 2.36e-01] [lr: 8.93e-04][max mem allocated: 3.15e+04] (2387.3 ms)
INFO:root:[27,   512/ 1053] - train_loss: 0.2106 -[wd: 2.37e-01] [lr: 8.89e-04][max mem allocated: 3.15e+04] (2387.5 ms)
slurmstepd: error: *** JOB 5995 ON hgx CANCELLED AT 2025-05-14T12:02:44 ***
