INFO:root:called-params config/configs.yaml
INFO:root:loaded params...
INFO:root:{'k_means': {'K': 7806}, 'patches': {'N': [16, 32, 64, 80, 128]}, 'num_workers': 32, 'base_dir': '/home/rtcalumby/adam/luciano/PlantCLEF2025/PlantCLEF2025/', 'pretrained_path': 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar', 'data': {'class_mapping': 'pretrained_models/class_mapping.txt', 'species_mapping': 'pretrained_models/species_id_to_name.txt', 'test_data': '/home/rtcalumby/adam/luciano/PlantCLEF2025/test_dataset/'}}
INFO:root:Running... (rank: 0/4)
INFO:root:Initialized (rank/world-size) 0/4
<class 'torch.cuda.device'>
INFO:timm.models._helpers:Loaded state_dict_ema from checkpoint 'pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar'
<class 'torch.cuda.device'>
Test dataset created
<class 'torch.cuda.device'>
Test dataset created
Test dataset created
<class 'torch.cuda.device'>
Test dataset created
INFO:root:Image [1/2105]
INFO:root:Preprocessed batch no. 1
Rank no. %d 0
INFO:root:Preprocessed batch no. 2
Rank no. %d 0
INFO:root:Test gathering
[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9ec7fd6897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f9ec92a3252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f9ec92a8070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f9ec92a93bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f9f29841b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f9f326f2609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f9f324bd353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600036 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9ec7fd6897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f9ec92a3252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f9ec92a8070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f9ec92a93bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f9f29841b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f9f326f2609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f9f324bd353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9ec7fd6897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe2c9e9 (0x7f9ec8f2c9e9 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3b65 (0x7f9f29841b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f9f326f2609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f9f324bd353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E ProcessGroupNCCL.cpp:563] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
[rank0]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 0] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank0]:[E ProcessGroupNCCL.cpp:577] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E ProcessGroupNCCL.cpp:583] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc105d1e897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7fc106feb252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7fc106ff0070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7fc106ff13bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7fc167589b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc17043a609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc170205353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600039 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc105d1e897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7fc106feb252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7fc106ff0070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7fc106ff13bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7fc167589b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7fc17043a609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7fc170205353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fc105d1e897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe2c9e9 (0x7fc106c749e9 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3b65 (0x7fc167589b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7fc17043a609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7fc170205353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[E ProcessGroupNCCL.cpp:563] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 3] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank3]:[E ProcessGroupNCCL.cpp:577] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:583] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f78bbc67897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f78bcf34252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f78bcf39070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f78bcf3a3bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f791d4d2b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f7926383609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f792614e353 in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f248fe06897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f24910d3252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f24910d8070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f24910d93bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f24f1671b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f24fa522609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f24fa2ed353 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError  what():  '
[PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f248fe06897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f24910d3252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f24910d8070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f24910d93bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f24f1671b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f24fa522609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f24fa2ed353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f248fe06897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe2c9e9 (0x7f2490d5c9e9 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3b65 (0x7f24f1671b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f24fa522609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f24fa2ed353 in /lib/x86_64-linux-gnu/libc.so.6)
  what():  
[PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=BROADCAST, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600074 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f78bbc67897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f78bcf34252 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f78bcf39070 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f78bcf3a3bc in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x7f791d4d2b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x8609 (0x7f7926383609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f792614e353 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/conda/conda-bld/pytorch_1712608885084/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f78bbc67897 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe2c9e9 (0x7f78bcbbd9e9 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3b65 (0x7f791d4d2b65 in /home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #3: <unknown function> + 0x8609 (0x7f7926383609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #4: clone + 0x43 (0x7f792614e353 in /lib/x86_64-linux-gnu/libc.so.6)

/home/rtcalumby/miniconda3/envs/fgdcc/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 80 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
